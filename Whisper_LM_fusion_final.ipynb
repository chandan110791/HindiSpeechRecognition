{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandan110791/HindiSpeechRecognition/blob/main/Whisper_LM_fusion_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtnWp1Z9NIrh"
      },
      "source": [
        "# Fusing LM with Whisper for lower WER\n",
        "The aim is to fuse a BPE-level LM scores with the generated tokens scores while beam-search decoding in Whisper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWD69559NPaz"
      },
      "source": [
        "## **MILESTONE 1**:\n",
        "Instantiate a Language Model to be integrated with Whisper.\n",
        "The chosen LM is an n-gram language model, trained with [KenLM](https://github.com/kpu/kenlm) library.\n",
        "\n",
        "### Step 1:\n",
        "Write code to run an already available LM in a standalone manner and be able to give a score any input sequence.\n",
        "**Chosen Model**: [Riva ASR Hindi LM](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_hi_in_lm/files?version=deployable_v3.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5ZcXEbPFgD"
      },
      "source": [
        "Download and build the KenLM toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOD5vBabeTED",
        "outputId": "e245aa24-055e-4895-e543-8bc2052c8c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-13 11:27:39--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491888 (480K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 480.36K  1.67MB/s    in 0.3s    \n",
            "\n",
            "2023-11-13 11:27:40 (1.67 MB/s) - written to stdout [491888/491888]\n",
            "\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
            "-- Found Threads: TRUE  \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")  \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.5\") \n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Configuring done (1.3s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 50%] Built target probing_hash_table_benchmark\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 57%] Built target kenlm_filter\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 75%] Built target fragment\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 77%] Built target query\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 86%] Built target kenlm_benchmark\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 91%] Built target kenlm_builder\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 93%] Built target filter\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 96%] Built target phrase_table_vocab\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n",
            "build_binary  filter\tkenlm_benchmark  phrase_table_vocab\t       query\n",
            "count_ngrams  fragment\tlmplz\t\t probing_hash_table_benchmark\n"
          ]
        }
      ],
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
        "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
        "!ls kenlm/build/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQgYC5RKfKHp"
      },
      "source": [
        "Download the KenLM Python library\n",
        "\n",
        "❗❗**Don't forget to restart the runtime after running this cell** ❗❗\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C3lwRxNebYv",
        "outputId": "e2f9d66a-0bcc-4f1f-ab47-43bffd8fe3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.6/553.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184345 sha256=6cc9a5db8180947baa0ae0b8fd35c040b77ff8e307bf437b2fafd6295101aa79\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9az3r8uo/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJviLM3FcWAB"
      },
      "source": [
        "## ***Pointer 1:Please add comments on the model being downloaded and whats its purpose ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr61mDKxZru1"
      },
      "source": [
        "Downloading the Hindi ASR n-gram language model from Nvidia which can be found [here](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/speechtotext_hi_in_lm/files?version=deployable_v3.1)\n",
        "\n",
        "This will be used for fusion with Whisper.\n",
        "I uploaded the binary version to a shareable location in my Gdrive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "NU0mgmnmhsxe",
        "outputId": "6484130f-7253-4276-9d29-58098fb08827"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\n",
            "To: /content/language_model_3p0.bin\n",
            "100%|██████████| 280M/280M [00:01<00:00, 147MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'language_model_3p0.bin'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the binary LM from Gdrive\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\"\n",
        "output = \"language_model_3p0.bin\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5LXkmNaklfx",
        "outputId": "151e2382-782e-41f4-8c8d-316b4fa00a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a 4-gram model\n"
          ]
        }
      ],
      "source": [
        "import kenlm\n",
        "model = kenlm.LanguageModel('/content/language_model_3p0.bin')\n",
        "print(\"This is a {}-gram model\".format(model.order))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fo71Q-qbfuU",
        "outputId": "a853e9f9-0f9d-4ed4-eb86-a3fae25a457d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-20.935253143310547 -21.663846969604492\n"
          ]
        }
      ],
      "source": [
        "# Below are 2 pairs of sentences that sound exactly the same in hindi but one of them is incorrect (lexically or semantically)\n",
        "# Generated using Bing Chat\n",
        "book_correct = \"मुझे यह किताब पसंद है।\"\n",
        "book_incorrect = \"मुझे यह किताब पसन्द है।\"\n",
        "\n",
        "correct_score = model.score(book_correct)\n",
        "incorrect_score = model.score(book_incorrect)\n",
        "assert correct_score > incorrect_score\n",
        "print(correct_score, incorrect_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDslvQ98bhFd",
        "outputId": "335af234-ca71-441e-f1c2-d51c10524ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-19.827430725097656 -22.76061248779297\n"
          ]
        }
      ],
      "source": [
        "sings_correct = \"वह बहुत अच्छा गाता है।\"\n",
        "sings_incorrect = \"वह बहुत अच्छा घाता है।\"\n",
        "\n",
        "\n",
        "correct_score = model.score(sings_correct)\n",
        "incorrect_score = model.score(sings_incorrect)\n",
        "assert correct_score > incorrect_score\n",
        "print(correct_score, incorrect_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt69WiP8dIJV"
      },
      "source": [
        "## ***Pointer 2:Please elaborate on the model being downloaded and whats its purpose . Also , why are we doing it two times  ***\n",
        "\n",
        "This is unnecessary and can be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "SJcuIrcggxHt",
        "outputId": "f2eafece-e032-484d-d4bf-7ad71cfd5cb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-4xQ3YCtsyONtpccGjOD1s9FtHqBX7RL\n",
            "To: /content/language_model_3p0.arpa\n",
            "100%|██████████| 3.18G/3.18G [00:33<00:00, 95.6MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'language_model_3p0.arpa'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # download the original arpa LM file for inspection\n",
        "# url = \"https://drive.google.com/uc?id=1-4xQ3YCtsyONtpccGjOD1s9FtHqBX7RL\"\n",
        "# output = \"language_model_3p0.arpa\"\n",
        "# gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wuzy3sEdTsq"
      },
      "source": [
        "$$:Please elaborate if we need to prepare a new arpa file for our model\n",
        "\n",
        "We can use this model for the time being, and finetune it later with the text data from our training set.\n",
        "However, I think it's not likely to come up with a better model than this, as this model is built by a team in Nvidia and it's probable that our training set is included in training this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43gXFE7Sg8kU",
        "outputId": "86667dff-df12-4421-d17b-1d376ffebb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.2548329\t<s> दोनों ही झल्लाये\n",
            "-0.2760634\t<s> चौधरी के अशुभचिंतकों\n",
            "-0.04973512\tडालियों पर बैठी शुकमंडली\n",
            "-0.07060567\tमनुष्यों को उन्हें बेमुरौवत\n",
            "-0.049646165\tऔर कड़क कर बोलेमेरी\n",
            "-0.04038189\tनिराश हो कर कहानहीं\n",
            "-0.08863469\tपड़ते ही वह अव्यवस्थितचित्त\n",
            "-0.19321889\tदोनों पक्षों से सवालजवाब\n",
            "-0.051110353\tझगड़ू साहु ने कहासमझू\n",
            "-0.20675866\tकरें तो उनकी भलमनसी\n",
            "-0.04876329\tनीति को सराहता थाइसे\n",
            "-0.06436408\t<s> मित्रता की मुरझायी\n",
            "-0.23735626\tकी गहराई से उपजतें\n",
            "-0.17502813\tपूर्णता की ओर बढातें\n",
            "-0.18197767\tजहाँ से अच्छा हिन्दोसिताँ\n",
            "-0.04437429\tहैं इसकी यह गुलसिताँ\n",
            "-0.06926097\tसंतरी हमारा वह पासबाँ\n",
            "-0.09434804\tजिनके दम से रश्कएजनाँ\n",
            "\n",
            "\\end\\\n"
          ]
        }
      ],
      "source": [
        "# inspect the last 20 lines inside the LM source\n",
        "!tail -20 language_model_3p0.arpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQA6huWockMP"
      },
      "outputs": [],
      "source": [
        "# some useful KenLM commands for future reference\n",
        "# generate binary\n",
        "# !kenlm/build/bin/build_binary dataset_tokenized_3gram.arpa dataset_tokenized_3gram.binary\n",
        "# create a new LM\n",
        "# !kenlm/build/bin/lmplz -o 3 --text dataset_tokenized.txt --arpa dataset_tokenized_3gram.arpa --discount_fallback\n",
        "# !tail -20 dataset_tokenized_3gram.arpa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGlHlhrffpHT"
      },
      "source": [
        "# Integrating the LM with Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKmnxRsofoXu",
        "outputId": "b9938565-d8e9-4d25-e8c4-7262cabc01c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/chandan110791/hindiWhisper/archive/master.zip\n",
            "  Downloading https://github.com/chandan110791/hindiWhisper/archive/master.zip\n",
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m7.4 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton==2.0.0 (from openai-whisper==20231106)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231106)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20231106) (3.27.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20231106) (3.13.1)\n",
            "Collecting lit (from triton==2.0.0->openai-whisper==20231106)\n",
            "  Downloading lit-17.0.5.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231106) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231106) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231106) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (2023.6.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from openai-whisper==20231106)\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m963.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch (from openai-whisper==20231106)\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper==20231106) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper==20231106) (0.41.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231106) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231106) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper, lit\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231106-py3-none-any.whl size=799894 sha256=5cd706d6cdee8f6830cf056e88a1d63ee3addf9ed9f5475756fcf6de1929ec39\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wakv1z_z/wheels/7e/59/d2/1662a1f0ae2217e2cd01935b4ac823b4eade5c382bed87a164\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.5-py3-none-any.whl size=93256 sha256=e3f1120ba7d88a2773bd61ab328bf198c9f34c030eacff15beb2c74d23a483f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/87/8e/5a42c0d4be23362b68bbff33b17f3c35a3df44f1cd2f5a24b4\n",
            "Successfully built openai-whisper lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, tiktoken, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, openai-whisper\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-17.0.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 openai-whisper-20231106 tiktoken-0.5.1 torch-2.0.1 triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install openai-whisper\n",
        "!pip install https://github.com/chandan110791/hindiWhisper/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28qXwUAIhQmA"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import torch\n",
        "import kenlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h061i7Dhaeq",
        "outputId": "d6884838-d9bf-4a41-bef8-d78b259d8cfd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 461M/461M [00:04<00:00, 112MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWRBxk5khiKQ",
        "outputId": "a95ab83a-f2f9-4e97-cd17-cfa061aa7d15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kKeSvrZo8z5Rsp1q-h3GXpG7vHctKMcG\n",
            "To: /content/sample.wav\n",
            "100%|██████████| 197k/197k [00:00<00:00, 77.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download a sample audio file from Gdrive\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1kKeSvrZo8z5Rsp1q-h3GXpG7vHctKMcG\"\n",
        "output = \"sample.wav\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "transcription = \"ब्रूड बॉक्स लैंगस्ट्रॉथ छत्ते का एक अनिवार्य हिस्सा है।\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOZKw5wGlNWm"
      },
      "outputs": [],
      "source": [
        "audio = whisper.load_audio(\"/content/sample.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kd1uG_bTTeSC"
      },
      "outputs": [],
      "source": [
        "# Download a sample audio file from Gdrive\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1kKeSvrZo8z5Rsp1q-h3GXpG7vHctKMcG\"\n",
        "output = \"sample.wav\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "transcription = \"ब्रूड बॉक्स लैंगस्ट्रॉथ छत्ते का एक अनिवार्य हिस्सा है।\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDH0eYJ8TkU4"
      },
      "outputs": [],
      "source": [
        "audio = whisper.load_audio(\"/content/sample.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "options = whisper.DecodingOptions(fp16 = False, beam_size=5, without_timestamps=True, language=\"hi\")\n",
        "result = whisper.decode(model, mel, options)\n",
        "baseline = result.text\n",
        "baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmbBreqNX3IX"
      },
      "source": [
        "## Baseline Whisper\n",
        "Without nbest or LM integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LFz_dwKdlXyR",
        "outputId": "37969971-4d76-4717-f7c7-53447af2d18c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "options = whisper.DecodingOptions(fp16 = False, beam_size=5, without_timestamps=True, language=\"hi\")\n",
        "result = whisper.decode(model, mel, options)\n",
        "baseline = result.text\n",
        "baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uew0igzbYPO"
      },
      "source": [
        "# Decoding with LM integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wAsYJNj3blQO",
        "outputId": "222b37f8-62be-4e2b-b86a-c7b1917097c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\n",
            "To: /content/language_model_3p0.bin\n",
            "100%|██████████| 280M/280M [00:04<00:00, 67.3MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'language_model_3p0.bin'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# adding the LM\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\"\n",
        "output = \"language_model_3p0.bin\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEluDSiWd2iC"
      },
      "source": [
        "$$:\n",
        "\n",
        "## ***Pointer 4: Getting an error here while trying to run the code . Elaborate a bit on the what happening here . Can you please refer to the function with changes done  ***\n",
        "\n",
        "The error was due to the fact that the imported whisper here is the original one, which doesn't include the changed that we did to the decoding options.\n",
        "\n",
        "Replaced this:\n",
        "```\n",
        "!pip install openai-whisper\n",
        "```\n",
        "with this:\n",
        "```\n",
        "!pip install https://github.com/chandan110791/hindiWhisper/archive/master.zip\n",
        "```\n",
        "\n",
        "and the error is fixed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEVX9bKV9BaR"
      },
      "outputs": [],
      "source": [
        "options = whisper.DecodingOptions(fp16 = False, withlm=True, beam_size=5,\n",
        "        patience=1.0, lm_path=\"/content/language_model_3p0.bin\", lm_alpha=1.0, lm_beta=0.0,\n",
        "        without_timestamps=True, language=\"hi\")\n",
        "decoding_withLM = whisper.decode(model, mel, options)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BPtkRefB-FUv",
        "outputId": "7bd1f8b6-1eb1-4066-f59d-95a6917af9aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoding_withLM.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTzCM2xRb28i"
      },
      "source": [
        "# Decoding with nbest (beam search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF6AnpXkb5XM",
        "outputId": "a314a338-bb04-440b-c8ff-48efabf24b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है -0.45291578358617324\n",
            "ब्रूद बाँक्ष लांश्टोट छत्ते का एक अनिवार्य हिस्सा है -0.45523316981428763\n",
            "ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है. -0.47296941078315347\n",
            "ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा हैं -0.48318856449450476\n",
            "ब्रूद बाँक्ष लांश्टोट छत्टे का एक अनिवार्य हिस्सा है -0.46669308344523114\n"
          ]
        }
      ],
      "source": [
        "options = whisper.DecodingOptions(fp16 = False, beam_size=5, return_nbest = True, without_timestamps=True, language=\"hi\")\n",
        "nbest = whisper.decode(model, mel, options)\n",
        "\n",
        "\n",
        "for candidate in nbest:\n",
        "  print(candidate.text, candidate.avg_logprob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J38npjnkcDW7"
      },
      "source": [
        "### Adding LM rescoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23wLneazBit1",
        "outputId": "072daaa5-d1c2-4466-fa8d-f91450ef2d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a 4-gram model\n"
          ]
        }
      ],
      "source": [
        "lm_model = kenlm.LanguageModel('/content/language_model_3p0.bin')\n",
        "print(\"This is a {}-gram model\".format(lm_model.order))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAdRnI_UBzWJ",
        "outputId": "44cedf0d-4d24-4c0d-fdfe-6cd16a271c6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है',\n",
              "  -0.45291578358617324,\n",
              "  -36.06303787231445),\n",
              " ('ब्रूद बाँक्ष लांश्टोट छत्ते का एक अनिवार्य हिस्सा है',\n",
              "  -0.45523316981428763,\n",
              "  -36.06303787231445),\n",
              " ('ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है.',\n",
              "  -0.47296941078315347,\n",
              "  -44.10427474975586),\n",
              " ('ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा हैं',\n",
              "  -0.48318856449450476,\n",
              "  -37.30411148071289),\n",
              " ('ब्रूद बाँक्ष लांश्टोट छत्टे का एक अनिवार्य हिस्सा है',\n",
              "  -0.46669308344523114,\n",
              "  -37.54087829589844)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nbest_with_lm_score = [(c.text, c.avg_logprob, lm_model.score(c.text)) for c in nbest]\n",
        "nbest_with_lm_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUBQ-WVCCH1N",
        "outputId": "7b430fe5-3f6c-4549-a8a0-955bf8efc6a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है', -0.8135461623093178),\n",
              " ('ब्रूद बाँक्ष लांश्टोट छत्ते का एक अनिवार्य हिस्सा है', -0.8158635485374321),\n",
              " ('ब्रूद बाँक्ष लांश्टोट छत्टे का एक अनिवार्य हिस्सा है', -0.8421018664042155),\n",
              " ('ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा हैं', -0.8562296793016337),\n",
              " ('ब्रूद बाँक्ष लंश्टोट छत्ते का एक अनिवार्य हिस्सा है.', -0.9140121582807121)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_weight = 0.01\n",
        "combined_scores = [(text, whisper_score + lm_score*lm_weight) for text, whisper_score, lm_score in nbest_with_lm_score]\n",
        "combined_scores.sort(key=lambda t: t[1], reverse=True)\n",
        "combined_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh5OtW5kz7gU"
      },
      "source": [
        "# Decoding with nbest (best of N hypothesis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rof-6s_mykVX"
      },
      "outputs": [],
      "source": [
        "options = whisper.DecodingOptions(fp16 = False, best_of=10, return_nbest=True, without_timestamps=True, temperature=0.3, language=\"hi\")\n",
        "nbest_best_of_n_hyp = whisper.decode(model, mel, options)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuDcmFH106lC",
        "outputId": "cded2418-b2e9-4e0f-c99f-a6d14937f715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ब्रूद बाँच लंश्टोट छत्ते का एक अनिवार्य हिस्सा है -0.45742596898760113\n",
            "ब्रूद बाओ्छ लंश्टोट छत्ते का एक अनिवार्ये हिस्चा है -0.5202431113032971\n",
            "ब्रूड़ ब्रूड़ लंच्टोट चद्ते का एक अनीवार्य लिए हिस्चा है -0.5065439448637121\n",
            "ब्रुध बाँच लंश्टोट चते का एक अनिवार्य हिस morally part of the Landstorch Chhattey. -0.9538334877260269\n",
            "ब्रुद बाँच लांश्टोट छद्टे का एक अनिवार्य हिस्चा है -0.5101523081461589\n",
            "ब्रूद बाँच लंच्टोड छत्टे का एक अनिवार्य लिए हिस्सा है -0.4865361798194147\n",
            "ब्रूद बाँच लांश्टोट छत्ते का एक अनीवार्य हिस्था है -0.49694071144893254\n",
            "ब्रूद बाँच लन्च्टोट चते का एक अनिवार्य हिस्चा है -0.5291237149919782\n",
            "ब्रूद भोग्ष लंग स्थ्टोड शथ्टे का एक अनिवार्य हिस्सा है -0.5426437135726686\n",
            "ब्रूद बाँच लंश्टोट छद्टे का एक अनिवारे हिस्चा है -0.49345003325363684\n"
          ]
        }
      ],
      "source": [
        "for candidate in nbest_best_of_n_hyp:\n",
        "  print(candidate.text, candidate.avg_logprob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOXOEofa2ZAe",
        "outputId": "980fca20-e1d0-4608-9c31-8ed779ab9a43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ब्रूद बाँच लंश्टोट छत्ते का एक अनिवार्य हिस्सा है',\n",
              "  -0.45742596898760113,\n",
              "  -35.82866668701172),\n",
              " ('ब्रूद बाओ्छ लंश्टोट छत्ते का एक अनिवार्ये हिस्चा है',\n",
              "  -0.5202431113032971,\n",
              "  -49.43550491333008),\n",
              " ('ब्रूड़ ब्रूड़ लंच्टोट चद्ते का एक अनीवार्य लिए हिस्चा है',\n",
              "  -0.5065439448637121,\n",
              "  -54.91597366333008),\n",
              " ('ब्रुध बाँच लंश्टोट चते का एक अनिवार्य हिस morally part of the Landstorch Chhattey.',\n",
              "  -0.9538334877260269,\n",
              "  -81.96270751953125),\n",
              " ('ब्रुद बाँच लांश्टोट छद्टे का एक अनिवार्य हिस्चा है',\n",
              "  -0.5101523081461589,\n",
              "  -46.626407623291016),\n",
              " ('ब्रूद बाँच लंच्टोड छत्टे का एक अनिवार्य लिए हिस्सा है',\n",
              "  -0.4865361798194147,\n",
              "  -46.603515625),\n",
              " ('ब्रूद बाँच लांश्टोट छत्ते का एक अनीवार्य हिस्था है',\n",
              "  -0.49694071144893254,\n",
              "  -49.201133728027344),\n",
              " ('ब्रूद बाँच लन्च्टोट चते का एक अनिवार्य हिस्चा है',\n",
              "  -0.5291237149919782,\n",
              "  -46.626407623291016),\n",
              " ('ब्रूद भोग्ष लंग स्थ्टोड शथ्टे का एक अनिवार्य हिस्सा है',\n",
              "  -0.5426437135726686,\n",
              "  -42.747047424316406),\n",
              " ('ब्रूद बाँच लंश्टोट छद्टे का एक अनिवारे हिस्चा है',\n",
              "  -0.49345003325363684,\n",
              "  -50.678977966308594)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_model = kenlm.LanguageModel('/content/language_model_3p0.bin')\n",
        "nbest_with_lm_score = [(c.text, c.avg_logprob, lm_model.score(c.text)) for c in nbest_best_of_n_hyp]\n",
        "nbest_with_lm_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIuqEbJ-2d-k",
        "outputId": "bb2afb82-8baa-45ec-a4cd-c152175916cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('ब्रूद बाँच लंश्टोट छत्ते का एक अनिवार्य हिस्सा है', -0.8157126358577182),\n",
              " ('ब्रूद बाँच लंच्टोड छत्टे का एक अनिवार्य लिए हिस्सा है',\n",
              "  -0.9525713360694148),\n",
              " ('ब्रूद भोग्ष लंग स्थ्टोड शथ्टे का एक अनिवार्य हिस्सा है',\n",
              "  -0.9701141878158327),\n",
              " ('ब्रुद बाँच लांश्टोट छद्टे का एक अनिवार्य हिस्चा है', -0.976416384379069),\n",
              " ('ब्रूद बाँच लांश्टोट छत्ते का एक अनीवार्य हिस्था है', -0.9889520487292061),\n",
              " ('ब्रूद बाँच लन्च्टोट चते का एक अनिवार्य हिस्चा है', -0.9953877912248883),\n",
              " ('ब्रूद बाँच लंश्टोट छद्टे का एक अनिवारे हिस्चा है', -1.0002398129167227),\n",
              " ('ब्रूद बाओ्छ लंश्टोट छत्ते का एक अनिवार्ये हिस्चा है', -1.014598160436598),\n",
              " ('ब्रूड़ ब्रूड़ लंच्टोट चद्ते का एक अनीवार्य लिए हिस्चा है',\n",
              "  -1.0557036814970129),\n",
              " ('ब्रुध बाँच लंश्टोट चते का एक अनिवार्य हिस morally part of the Landstorch Chhattey.',\n",
              "  -1.7734605629213394)]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_weight = 0.01\n",
        "combined_scores_bestofNSampling = [(text, whisper_score + lm_score*lm_weight) for text, whisper_score, lm_score in nbest_with_lm_score]\n",
        "combined_scores_bestofNSampling.sort(key=lambda t: t[1], reverse=True)\n",
        "combined_scores_bestofNSampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcAl6kVkuSSw"
      },
      "source": [
        "# **RUN THE NOTEBOOK STARTING HERE**\n",
        "\n",
        "# The next sections include:\n",
        "- Importing a finetuned huggingface model to our codebase\n",
        "- Running the evaluation of the 4 decoding variants on the imported dataset\n",
        "- Computing the WER and CER for each of the decoding variants\n",
        "\n",
        "## The decoding strategies are:\n",
        "1. baseline decoding without LM\n",
        "2. Deep fusion of the LM with the token probabilities during beam search decoding (try to find the optimal value for lm_alpha)\n",
        "3. Shallow fusion by rescoring the N best candidates generated through beam search (try to find the optimal value for lm_weight)\n",
        "4. Shallow fusion by rescoring the N best candidates generated through greedy decoding using best of N sampling (try to find the optimal value for lm_weight and temperature)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zRMuGzF1WX8",
        "outputId": "8e81be72-152a-4a5f-ee4c-af4b9767e22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/chandan110791/hindiWhisper/archive/master.zip\n",
            "  Downloading https://github.com/chandan110791/hindiWhisper/archive/master.zip\n",
            "\u001b[2K     \u001b[32m/\u001b[0m \u001b[32m7.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting triton==2.0.0 (from openai-whisper==20231106)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231106) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231106)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20231106) (3.27.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper==20231106) (3.13.1)\n",
            "Collecting lit (from triton==2.0.0->openai-whisper==20231106)\n",
            "  Downloading lit-17.0.5.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231106) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231106) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231106) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231106) (2023.6.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch (from openai-whisper==20231106)\n",
            "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch (from openai-whisper==20231106)\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch->openai-whisper==20231106)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper==20231106) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper==20231106) (0.41.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231106) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231106) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231106) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper, lit\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231106-py3-none-any.whl size=799899 sha256=be30122c641d2e4145f6d0b079d755b0374af774fdc2954397f16334cad5290f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-acf74m4_/wheels/7e/59/d2/1662a1f0ae2217e2cd01935b4ac823b4eade5c382bed87a164\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.5-py3-none-any.whl size=93256 sha256=7083e876cabe129ff762df0e24ab93ec9ec05b1b798e834f02879a731b79c85f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/87/8e/5a42c0d4be23362b68bbff33b17f3c35a3df44f1cd2f5a24b4\n",
            "Successfully built openai-whisper lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, tiktoken, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, openai-whisper\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-17.0.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 openai-whisper-20231106 tiktoken-0.5.1 torch-2.0.1 triton-2.0.0\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[2K     \u001b[32m-\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp310-cp310-linux_x86_64.whl size=3184344 sha256=bcd67c1b8f526b68bddc0f227b79114c01abd2f4b89c8a2a0f2ad81286637d54\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8o8rguzk/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-fi0b1nl8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-fi0b1nl8\n",
            "  Resolved https://github.com/huggingface/transformers to commit c13a43aaf2194b5d05272af5c9c7e394688b72df\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8049135 sha256=5b7d13de7e43fab6558b5f831d4048111faf1d5411b1be154a21f7d636ed2fa2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zunqm478/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.36.0.dev0\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.3 rapidfuzz-3.5.2\n",
            "Collecting gradio\n",
            "  Downloading gradio-4.7.1-py3-none-any.whl (16.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.0 (from gradio)\n",
            "  Downloading gradio_client-0.7.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.7/302.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.19.4)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio)\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: requests~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.31.0)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.5.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.7.0->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.0->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=4.0 (from gradio)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx->gradio)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=82243a786490d7e981c67a92f21a63c2fd3c73d4d5af5c52f1dbe9bab62c33c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, typing-extensions, tomlkit, shellingham, semantic-version, python-multipart, orjson, h11, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, httpcore, pydantic, httpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 fastapi-0.104.1 ffmpy-0.3.1 gradio-4.7.1 gradio-client-0.7.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 orjson-3.9.10 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.27.0 tomlkit-0.12.0 typing-extensions-4.8.0 uvicorn-0.24.0.post1 websockets-11.0.3\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.7)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/chandan110791/hindiWhisper/archive/master.zip\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip\n",
        "!pip install transformers\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "# !pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the binary LM from Gdrive\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\"\n",
        "output = \"language_model_3p0.bin\"\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Otxfl9LTGNej",
        "outputId": "e052863a-8c3c-4fc9-a5d2-a4fdbd9cc57e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\n",
            "To: /content/language_model_3p0.bin\n",
            "100%|██████████| 280M/280M [00:02<00:00, 93.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'language_model_3p0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9rpmw6hfbcf"
      },
      "source": [
        "$$\n",
        "\n",
        "$$:\n",
        "\n",
        "## ***Pointer 5: its looking for a file named as pytorchmodel.bin , it does not exists hence note  able to integrate the model : CKSINGH/whisper-small-hi-iiib tuned at : https://colab.research.google.com/github/chandan110791/HindiSpeechRecognition/blob/main/fine_tune_whisper_iiitb.ipynb#scrollTo=d7030622-caf7-4039-939b-6195cdaa2585  . Any inputs ***\n",
        "\n",
        "The reason for this is that the model file after finetuning must be pickled:\n",
        "so pytorch_model.bin should be found in this path:\n",
        "https://huggingface.co/CKSINGH/whisper-small-hi-iiib/tree/main\n",
        "\n",
        "Will use this one https://huggingface.co/sanchit-gandhi/whisper-small-hi/tree/main to make the required changes to the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXpUd4OoS3y"
      },
      "source": [
        "## In the next cell, you need to change the repo id to your repo after generating the pytorch_model.bin (pickle format of the finetuned model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "def resample_audio(input_file_path, output_file_path, orig_sr, target_sr):\n",
        "    \"\"\"\n",
        "    Resample an audio file from original sample rate to target sample rate.\n",
        "\n",
        "    Args:\n",
        "    - input_file_path (str): Path to the input audio file.\n",
        "    - output_file_path (str): Path where the resampled audio file will be saved.\n",
        "    - orig_sr (int): Original sample rate of the audio file.\n",
        "    - target_sr (int): Target sample rate to resample the audio file.\n",
        "    \"\"\"\n",
        "    # Load the audio file\n",
        "    audio, sr = librosa.load(input_file_path, sr=orig_sr)\n",
        "\n",
        "    # Resample to target sample rate\n",
        "    audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "    # Save the resampled audio\n",
        "    sf.write(output_file_path, audio_resampled, target_sr)\n"
      ],
      "metadata": {
        "id": "vehLOB9pCfdU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "b5b9cb9c276c4f75a02171493454991d",
            "d56c3eaa62a744609bafdca5acc038e2",
            "25487c440dc849659f612de9b9402617",
            "ca4e3baf7eea408286fea1780ef21ea3",
            "c9813f84bcf34d3b885b0d63c6b7ab4c",
            "17dd643568a64cbcbc5e29f32024ec65",
            "798de64a6ff849e098e4942920ee612e",
            "d31eabfda37e47c1a74cee038703a941",
            "94e6bc86395e4935a7265cd06b83e5d8",
            "40bc37638c7e41d9989dab6c84b7fd73",
            "f53ef68e08ea4507a477a69f09b85b16"
          ]
        },
        "id": "kMDX0Asjuc4e",
        "outputId": "33107bdb-e8ff-400a-9d35-caeca9aeb2d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5b9cb9c276c4f75a02171493454991d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/pytorch_model.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import whisper\n",
        "import kenlm\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "import torch\n",
        "import jiwer\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "# using pickle to serialize the map_dict\n",
        "import pickle\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "filename = \"pytorch_model.bin\"\n",
        "\n",
        "#hf_hub_download(repo_id=\"CKSINGH/whisper-small-hi-iiib\", filename=filename, local_dir=\"/content/\")\n",
        "#hf_hub_download(repo_id=\"CKSINGH/whisper-small-hi-firefox\", filename=filename, local_dir=\"/content/\")\n",
        "#hf_hub_download(repo_id=\"sanchit-gandhi/whisper-small-hi\", filename=filename, local_dir=\"/content/\")\n",
        "hf_hub_download(repo_id=\"CKSINGH/whisper-small-hi-graminVoice\", filename=filename, local_dir=\"/content/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKwYojN1ol9j"
      },
      "source": [
        "# Here you need to change the model size from \"small\" to \"medium\" if you finetuned the whisper-medium (as the one in this repo: https://huggingface.co/CKSINGH/whisper-small-hi-iiib/tree/main)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P2IO4Vy8u4GW"
      },
      "outputs": [],
      "source": [
        "# to enable verbose printing of exceptions (+ layers matching name)\n",
        "DEBUG = True\n",
        "\n",
        "# set to True if your custom model has been trained using DDP (multi-gpu)\n",
        "# as in my case, in the custom HF model, keys have a prefix (model.)\n",
        "# it should come from the fact that I have trained on a milti-gpu machine, using DDP\n",
        "DDP_TRAINED = True\n",
        "\n",
        "# if DDP we have to add a prefix to match with the HF state_dict\n",
        "if DDP_TRAINED:\n",
        "    PREFIX = \"model.\"\n",
        "else:\n",
        "    PREFIX = \"\"\n",
        "\n",
        "MODEL_SIZE = \"small\"\n",
        "\n",
        "# the device where you're running this code\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# the name of the file with your fine-tuned model\n",
        "FINETUNED_MODEL = \"pytorch_model.bin\"\n",
        "\n",
        "# the name of the file for the serialized map_dict\n",
        "# a different name, to avoid overwrite it\n",
        "FILE_DICT = MODEL_SIZE + \"_map_dict01.pkl\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q3vqObGaMSt9"
      },
      "outputs": [],
      "source": [
        "def import_hf_model(finetuned_model, model_size, device, file_dict, debug=True):\n",
        "\n",
        "  def has_numbers(inputString):\n",
        "      return any(char.isdigit() for char in inputString)\n",
        "\n",
        "  # next functions are used to make sanity checks for the mappings\n",
        "\n",
        "  # get if it is encoder or decoder\n",
        "  def extract_function(key_name):\n",
        "      # encoder or decoder is the first part of the key\n",
        "      first_part = key_name.split(\".\")[0]\n",
        "\n",
        "      key_func = None\n",
        "      if first_part in [\"enconder\", \"decoder\"]:\n",
        "          key_func = first_part\n",
        "\n",
        "      return key_func\n",
        "\n",
        "  def extract_layer_num(key_name):\n",
        "      # layer num is the third piece\n",
        "      layer_num = None\n",
        "\n",
        "      if has_numbers(key_name):\n",
        "          layer_num = key_name.split(\".\")[2]\n",
        "\n",
        "      return layer_num\n",
        "\n",
        "  # check that the two keys are for layers\n",
        "  # with the same function\n",
        "  # (both encoder or both decoder)\n",
        "  # and have the same layer number\n",
        "  # this way we are super-safe (I think)\n",
        "  def sanity_check(key1, key2):\n",
        "      is_ok = True\n",
        "\n",
        "      # check same func (encoder or decoder)\n",
        "      func1 = extract_function(key1)\n",
        "      func2 = extract_function(key2)\n",
        "\n",
        "      if func1 != func2:\n",
        "          print(f\"Warning: layers seem to have different functions: {key1},{key2}\")\n",
        "          is_ok = False\n",
        "\n",
        "      # check same layer_num\n",
        "      layer1 = extract_layer_num(key1)\n",
        "      layer2 = extract_layer_num(key2)\n",
        "\n",
        "      if layer1 != layer2:\n",
        "          print(f\"Warning: layers seem to have different numbers: {key1},{key2}\")\n",
        "          is_ok = False\n",
        "\n",
        "      return is_ok\n",
        "\n",
        "  if not os.path.isfile(file_dict):\n",
        "    # Vanilla means: not custom trained\n",
        "    print()\n",
        "    print(\"Loading vanilla Whisper model\")\n",
        "    model = whisper.load_model(model_size, device=device)\n",
        "\n",
        "    print(\"Loading vanilla HF Model\")\n",
        "    hugging_face_model = WhisperForConditionalGeneration.from_pretrained(\n",
        "        \"openai/whisper-\" + model_size\n",
        "    ).to(device)\n",
        "\n",
        "    # extract state-dict from both\n",
        "    state_d_openai = model.state_dict()\n",
        "    state_d_huggingface = hugging_face_model.model.state_dict()\n",
        "\n",
        "    # build the mapping between keys...\n",
        "    map_dict = {}\n",
        "    print(\"Matching layers...\")\n",
        "\n",
        "    # for every layer in OpenAI model\n",
        "    n_sanity_ok = 0\n",
        "\n",
        "    #\n",
        "    # here we're considering the cartesian product of the two state dict and try to match\n",
        "    # rules applied:\n",
        "    # 1. the two layers have the same shape\n",
        "    # 2. the two layer have the same parameters' values\n",
        "    # 3. we apply sanity check (see function above)\n",
        "    #\n",
        "    for k in tqdm(state_d_openai):\n",
        "        # find a layer in the HF model, check with j\n",
        "        for j in state_d_huggingface:\n",
        "            # where parameters have same shape and same values\n",
        "            if state_d_huggingface[j].shape == state_d_openai[k].shape:\n",
        "                if torch.all(torch.eq(state_d_huggingface[j], state_d_openai[k])).item():\n",
        "                    # found, register the mapping\n",
        "                    map_dict[k] = j\n",
        "                    # make some check and eventually print a warning\n",
        "                    if sanity_check(k, j) == True:\n",
        "                        n_sanity_ok += 1\n",
        "\n",
        "                        # if you enable thsi print you can see the name of the layer\n",
        "                        # chosen in the match and you will se that they have the same functions\n",
        "                        if debug:\n",
        "                            print(k, j)\n",
        "\n",
        "                    break\n",
        "\n",
        "\n",
        "    # check if we have matched every entry\n",
        "    print(\"Check if we have matched every entry in state_dict...\")\n",
        "    print()\n",
        "    print(f\"Number of keys: {len(map_dict.keys())}\")\n",
        "    assert len(map_dict.keys()) == len(state_d_openai.keys()), \"The match is not complete !\"\n",
        "\n",
        "    print(f\"Number of sanity_check ok: {n_sanity_ok}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Match is complete !!!\")\n",
        "    print()\n",
        "\n",
        "\n",
        "    # serialize the map_dict to file\n",
        "    print(\"Serializing map_dict...\")\n",
        "\n",
        "    with open(file_dict, \"wb\") as f:\n",
        "        pickle.dump(map_dict, f)\n",
        "        f.close()\n",
        "\n",
        "    print(f\"map_dict saved as: {file_dict}...\")\n",
        "    print()\n",
        "\n",
        "  else:\n",
        "    # loading with match keys\n",
        "    # restart from pickle file\n",
        "    print(\"Reloading map_dict...\")\n",
        "    print()\n",
        "    with open(file_dict, \"rb\") as f:\n",
        "        map_dict = pickle.load(f)\n",
        "\n",
        "  # loading fine-tuned dict\n",
        "  print(\"Loading fine tuned dict...\")\n",
        "\n",
        "  # added map_location to handle the fact that the custom model has been trained on GPU\n",
        "  state_dict_finetuned = torch.load(finetuned_model, map_location=torch.device(device))\n",
        "\n",
        "  print(state_dict_finetuned.keys())\n",
        "  # build the state_dict to be used\n",
        "  # take the key name from standard (OpenAI) and the value from finetuned (HF)\n",
        "  print(\"Rebuild the state dict...\")\n",
        "  new_state_dict = {}\n",
        "  n_except = 0\n",
        "  for k in tqdm(map_dict.keys()):\n",
        "      try:\n",
        "        # You must add \"model.\" if you have used DDP in custom training\n",
        "        # see DDP_TRAINED above\n",
        "        # PREFIX is added to a HF fine-tuned 8with DDP). It is not in vanulla HF models\n",
        "        new_state_dict[k] = state_dict_finetuned[PREFIX + map_dict[k]]\n",
        "      except Exception as ex:\n",
        "        n_except += 1\n",
        "\n",
        "        if debug:\n",
        "            print(\"exception\")\n",
        "            print(PREFIX + map_dict[k])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  msg_err = f\"Rebuild state dict failed, {n_except} pick failed\"\n",
        "  assert n_except == 0, msg_err\n",
        "\n",
        "\n",
        "\n",
        "  print()\n",
        "  print(\"Loading the final model...\")\n",
        "  model.load_state_dict(new_state_dict)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"\""
      ],
      "metadata": {
        "id": "SLQBKRmKNdWs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515,
          "referenced_widgets": [
            "856e2a72a2084ddc9d4085fd05a6e1ac",
            "3cfc3a3d32c64f348a0fca84302ea47d",
            "11d586734d9845aeade23b9caf832075",
            "882512b358964dcabc7e85d63b5c1b72",
            "4362a4fb3abf49079f3335e2881c063d",
            "9d488d59187a4022955587690b1fbdf7",
            "97b085fd630e4dacb189804b72b08803",
            "a49a0c1293984c5d8280e4666779ffa3",
            "f0375dd78dc34657ba930915e4d5989a",
            "0a13d1784af54f3c824f8b8727ca40ad",
            "622675dbed5f4527af81c876df8c2ae7",
            "2f3972ee7b0f434ba33cbf95eec21fa6",
            "aa4b88dedb4a4102a79486911876d4b8",
            "c24561369ad3479c8d1eb9e0a35e8c41",
            "79d310d010cd40aeb6352a8e5497844a",
            "639f139161e34d70854ca54d5f1881ab",
            "7199049a45fa4747b6304af5b439b99f",
            "252f2962a23d4114b4a48caaf7a07d64",
            "c8fe2a8f1413483fbaf9749ac5620aae",
            "a3c5eac8fc954826b134c065fa3413d0",
            "6957ee665f5b499893096a1be87a3c49",
            "41fea02c370f45479c55102068e40a4a",
            "de6fd611cf4e4f42a58ef38e297965eb",
            "c76d5552c90a45879b94413f385b7d4d",
            "9b2203caca2f4588899ea8902b3095b8",
            "74ea3499b452499bbb105e1ffc3759a9",
            "40481571c848462c825a108d588971ca",
            "a767f004a98942cbb0942122a7b24c8a",
            "f4a5b237bf9f4c738df63e75873a1ab9",
            "37c64521ad104a4aaea6725ef2b9f20f",
            "6102748e8e884ebe9b6b4d17dc2fe83d",
            "cc9a0bd9671b46b0a214b41273401ebf",
            "109ab10b8f704ff5bdc5d10c28a6ad00"
          ]
        },
        "id": "gCxqmfBJvE__",
        "outputId": "0fc39db0-4e77-42bf-9202-16d0a03b10eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading vanilla Whisper model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:07<00:00, 61.7MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vanilla HF Model\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "856e2a72a2084ddc9d4085fd05a6e1ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f3972ee7b0f434ba33cbf95eec21fa6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/3.84k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de6fd611cf4e4f42a58ef38e297965eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matching layers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 479/479 [00:01<00:00, 264.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if we have matched every entry in state_dict...\n",
            "\n",
            "Number of keys: 479\n",
            "Number of sanity_check ok: 479\n",
            "\n",
            "Match is complete !!!\n",
            "\n",
            "Serializing map_dict...\n",
            "map_dict saved as: small_map_dict01.pkl...\n",
            "\n",
            "Loading fine tuned dict...\n",
            "odict_keys(['model.encoder.conv1.weight', 'model.encoder.conv1.bias', 'model.encoder.conv2.weight', 'model.encoder.conv2.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layer_norm.weight', 'model.encoder.layer_norm.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.encoder_attn.k_proj.weight', 'model.decoder.layers.6.encoder_attn.v_proj.weight', 'model.decoder.layers.6.encoder_attn.v_proj.bias', 'model.decoder.layers.6.encoder_attn.q_proj.weight', 'model.decoder.layers.6.encoder_attn.q_proj.bias', 'model.decoder.layers.6.encoder_attn.out_proj.weight', 'model.decoder.layers.6.encoder_attn.out_proj.bias', 'model.decoder.layers.6.encoder_attn_layer_norm.weight', 'model.decoder.layers.6.encoder_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.encoder_attn.k_proj.weight', 'model.decoder.layers.7.encoder_attn.v_proj.weight', 'model.decoder.layers.7.encoder_attn.v_proj.bias', 'model.decoder.layers.7.encoder_attn.q_proj.weight', 'model.decoder.layers.7.encoder_attn.q_proj.bias', 'model.decoder.layers.7.encoder_attn.out_proj.weight', 'model.decoder.layers.7.encoder_attn.out_proj.bias', 'model.decoder.layers.7.encoder_attn_layer_norm.weight', 'model.decoder.layers.7.encoder_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.encoder_attn.k_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.weight', 'model.decoder.layers.8.encoder_attn.v_proj.bias', 'model.decoder.layers.8.encoder_attn.q_proj.weight', 'model.decoder.layers.8.encoder_attn.q_proj.bias', 'model.decoder.layers.8.encoder_attn.out_proj.weight', 'model.decoder.layers.8.encoder_attn.out_proj.bias', 'model.decoder.layers.8.encoder_attn_layer_norm.weight', 'model.decoder.layers.8.encoder_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.encoder_attn.k_proj.weight', 'model.decoder.layers.9.encoder_attn.v_proj.weight', 'model.decoder.layers.9.encoder_attn.v_proj.bias', 'model.decoder.layers.9.encoder_attn.q_proj.weight', 'model.decoder.layers.9.encoder_attn.q_proj.bias', 'model.decoder.layers.9.encoder_attn.out_proj.weight', 'model.decoder.layers.9.encoder_attn.out_proj.bias', 'model.decoder.layers.9.encoder_attn_layer_norm.weight', 'model.decoder.layers.9.encoder_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.encoder_attn.k_proj.weight', 'model.decoder.layers.10.encoder_attn.v_proj.weight', 'model.decoder.layers.10.encoder_attn.v_proj.bias', 'model.decoder.layers.10.encoder_attn.q_proj.weight', 'model.decoder.layers.10.encoder_attn.q_proj.bias', 'model.decoder.layers.10.encoder_attn.out_proj.weight', 'model.decoder.layers.10.encoder_attn.out_proj.bias', 'model.decoder.layers.10.encoder_attn_layer_norm.weight', 'model.decoder.layers.10.encoder_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.encoder_attn.k_proj.weight', 'model.decoder.layers.11.encoder_attn.v_proj.weight', 'model.decoder.layers.11.encoder_attn.v_proj.bias', 'model.decoder.layers.11.encoder_attn.q_proj.weight', 'model.decoder.layers.11.encoder_attn.q_proj.bias', 'model.decoder.layers.11.encoder_attn.out_proj.weight', 'model.decoder.layers.11.encoder_attn.out_proj.bias', 'model.decoder.layers.11.encoder_attn_layer_norm.weight', 'model.decoder.layers.11.encoder_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layer_norm.weight', 'model.decoder.layer_norm.bias', 'proj_out.weight'])\n",
            "Rebuild the state dict...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 479/479 [00:00<00:00, 1219096.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading the final model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = import_hf_model(finetuned_model=FINETUNED_MODEL, debug= False, model_size=MODEL_SIZE, device=DEVICE, file_dict=FILE_DICT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A4eGVNxPC5U7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c65148-4d45-4483-a85b-fac54f465100"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Whisper(\n",
              "  (encoder): AudioEncoder(\n",
              "    (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "    (blocks): ModuleList(\n",
              "      (0-11): 12 x ResidualAttentionBlock(\n",
              "        (attn): MultiHeadAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): TextDecoder(\n",
              "    (token_embedding): Embedding(51865, 768)\n",
              "    (blocks): ModuleList(\n",
              "      (0-11): 12 x ResidualAttentionBlock(\n",
              "        (attn): MultiHeadAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (cross_attn): MultiHeadAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=False)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (mlp_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "8536aa41d17347508a262bbc9693b7f8",
            "ba507efc55c145419acdec6f0fb62b2e",
            "f96cd823f757434997f49f4b2c7d2a04",
            "3f065f61e9e1443a85aee518670a4fa7",
            "ba67e8ff964d4073b870e1200a87fd4c",
            "ab9c06e19a154517b4ff46c7a52d29d4",
            "150486309a0540b0a51a96664f78aba7",
            "773b457557fd4352a317068d87fa4eef",
            "b805718a8b76423386511730e0fdeaff",
            "ab1471db6c574235ac5967dd80734786",
            "ff114fffeab840c6a8fab1662d1dddd7",
            "24c1c8f49f5a43eab9b1b9f9f0a12fc4",
            "e543d211badc4d41bd944df904ce0317",
            "d6ef36d77a09460d82ce46f28df66665",
            "c16fa1150d12439688fa21af50d995c7",
            "aed79d83bbe84ba5aa53f601588c4ee8",
            "28b858b805f54292a322d13611833c33",
            "7d5806a46c454e0c9c3f1ca5f70d4279",
            "455bae593a3240d9ba064c213a90ece9",
            "ba5939381af044029e254989149974db",
            "75a0a08417de42e88bdf8ffc693f42b6",
            "bb853deecb594dad80ae466afa531d75",
            "920da0787e654f6bb9a0c76cc88f0f5c",
            "bbb0d45741c8401b819941a9eda1c1a7",
            "b8b595673f6a4db4a47a25e88ebc2d4e",
            "981bc86f56014362942f1fad9d5fe00c",
            "a0f33f70e05d4897aac484cb940392d4",
            "4928327c81124c60bd8e3aca15942167",
            "7390656d2d4240969f6202876d1a57c7",
            "f0885c2a4abd4fe6902ca4f7aba7b2a1",
            "273a08df24d343a7a227d09b9e23ebb4",
            "3dd766dafea94135862b6e9e0d9876cf"
          ]
        },
        "id": "rasFuIlSix6U",
        "outputId": "32a54d14-6a4d-4a6a-803d-dd495eb93801"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8536aa41d17347508a262bbc9693b7f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# #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\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RGPaVW4glVi"
      },
      "source": [
        "$$:\n",
        "\n",
        "$$\n",
        "\n",
        "$$:\n",
        "\n",
        "## ***Pointer 6: We need to evaluate and calculate the WER,CER  on the Test data shared using our final model . Below are some boiler plates code to do the same  ***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-VwhKrCShjPP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "56ae7c0feb094c40a68dc1ade8fdae12",
            "5e39f5af40e7453988c77392971932b3",
            "cbdb0f2adacc466b8a1f38a968aca1b9",
            "05d139435d6b48729f0988faf15fcfe7",
            "6b780799ce1a45b6aaea60f24a29db37",
            "0da270b27fe44389958b37d944aca19a",
            "76b0e310458846e2b16bbf193f101fda",
            "a3acbd7a0c4346cba4109fcfdaf2d97b",
            "916092918e17497f834beadea72b51ec",
            "df70625253db4688b596586d5dbd276a",
            "edd2cc6165c34fc6a14a4add6b823f8b",
            "eda00a71cfe54282949579d9e3d49f4c",
            "1b5d25c07e054693a3984ad53598a20d",
            "4568fc83f44f4d3ea0556b47af11fa15",
            "cb922683c49f4c6ba7917a129c0e889a",
            "bd947ffa601e46e5a3c37b5510f0c01d",
            "75824b6cf5394fb3a5c93f7eff93800d",
            "d7230460e6924852aa0b8a4822341fca",
            "e03deeeb820344b3b34d9cc520cc2711",
            "8d1dee23754941eba223a76b4688b9cd",
            "d4289516c78a4ea4b7080dec1b8b1e33",
            "5086b292db6e4e1393df74aa1c4a7a38"
          ]
        },
        "outputId": "d1107895-2365-4aed-e562-3a3293e8cccf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/12.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ae7c0feb094c40a68dc1ade8fdae12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating txt.done.data.utf8 split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eda00a71cfe54282949579d9e3d49f4c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Prepare Test Data for calculating Wer and CER\n",
        "\n",
        "import datasets\n",
        "# import the load_dataset function\n",
        "from datasets import load_dataset\n",
        "\n",
        "# specify the URL directory and the data files\n",
        "# load the dataset from the URL directory\n",
        "\n",
        "\n",
        "datasets.config.DEFAULT_MAX_BATCH_SIZE = 10\n",
        "#integrated_LM_dataset_1 = load_dataset(\"/content/datadownload.py\")\n",
        "integrated_LM_dataset_2 = load_dataset(\"/content/datadownload_iith.py\")\n",
        "#integrated_LM_dataset_3 = load_dataset(\"/content/graminVoiceDatadownload.py\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "integrated_LM_dataset_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXPnPS-3AEXa",
        "outputId": "959fadcc-29aa-41c4-f177-0e441692f5da"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    txt.done.data.utf8: Dataset({\n",
              "        features: ['path', 'audio', 'sentence'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WEu6KbZ7jGuc"
      },
      "outputs": [],
      "source": [
        "#integrated_LM_dataset_1 = integrated_LM_dataset_1.remove_columns([\"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\",\"accents\",\"variant\"])\n",
        "integrated_LM_dataset_2 = integrated_LM_dataset_2.remove_columns([\"path\"])\n",
        "#integrated_LM_dataset_3 = integrated_LM_dataset_3.remove_columns([\"path\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5FfCeYtjqT3",
        "outputId": "0a554b77-e8d8-43ec-e695-b5a3f7dc6165"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    txt.done.data.utf8: Dataset({\n",
              "        features: ['audio', 'sentence'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "integrated_LM_dataset_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2lZFQrXajmrd"
      },
      "outputs": [],
      "source": [
        "#combined_dataset = datasets.concatenate_datasets([integrated_LM_dataset_1[\"test\"]])\n",
        "combined_dataset = datasets.concatenate_datasets([integrated_LM_dataset_2[\"txt.done.data.utf8\"]])\n",
        "#combined_dataset = datasets.concatenate_datasets([integrated_LM_dataset_3[\"text\"]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-sMtUIcj2Gj",
        "outputId": "b0e119e1-7db9-4984-9cfe-5f74cb5b4543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Dataset: 500 samples\n",
            "Test Dataset: 500 samples\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, concatenate_datasets, DatasetDict, load_metric\n",
        "\n",
        "# Define the split ratios\n",
        "#train_split = 0.75  # 80% of the data\n",
        "validation_split = 0.50  # 10% of the data\n",
        "test_split = 0.1  # 10% of the data\n",
        "\n",
        "# Compute the number of samples for each split\n",
        "num_samples = len(combined_dataset)\n",
        "num_validation = int(validation_split * num_samples)\n",
        "num_test = num_samples - num_validation  # Remaining 10%\n",
        "\n",
        "# Split the combined dataset\n",
        "validation_dataset = combined_dataset.select(indices=list(range(num_validation)))\n",
        "test_dataset = combined_dataset.select(indices=list(range(num_validation, num_samples)))\n",
        "\n",
        "# If you want to organize the split datasets in a DatasetDict for convenience:\n",
        "split_test_datasets = DatasetDict({\n",
        "    'validation': validation_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# Verify the resulting datasets\n",
        "print(f'Validation Dataset: {len(validation_dataset)} samples')\n",
        "print(f'Test Dataset: {len(test_dataset)} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADERPSIplyje"
      },
      "outputs": [],
      "source": [
        "## you can use only 10% of the data to perform evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmZe8mxxkrHG"
      },
      "source": [
        "Evaluation Metrics on the Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#correct the sampling rate"
      ],
      "metadata": {
        "id": "Wm50URNVlxld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "T5Zpz_I3qpQM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class customDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
        "    It will drop the last few seconds of a very small portion of the utterances.\n",
        "    \"\"\"\n",
        "    original_sample_rate = 48000  # 48kHz\n",
        "    target_sample_rate = 16000    # 16kHz\n",
        "\n",
        "    def __init__(self, dataset, device=DEVICE):\n",
        "        self.dataset = dataset\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
        "        # assert sample_rate == 16000\n",
        "        # audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
        "        # mel = whisper.log_mel_spectrogram(audio)\n",
        "        audio = self.dataset[item]['audio']\n",
        "        sentence = self.dataset[item]['sentence']\n",
        "        path = audio['path']\n",
        "        original_sample_rate = 48000  # 48kHz\n",
        "        target_sample_rate = 16000    # 16kHz\n",
        "        resample_audio(path, path, original_sample_rate, target_sample_rate)\n",
        "        array = audio['array']\n",
        "        sampling_rate = audio['sampling_rate']\n",
        "        audio = whisper.load_audio(path)\n",
        "        audio = whisper.pad_or_trim(audio)\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "        return (mel, sentence)\n",
        "\n",
        "dataset = customDataset(split_test_datasets[\"test\"])\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "id": "MELFhd-MZmBl",
        "outputId": "bbe626f7-4022-4ec7-d303-9da5b4dedbb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.1111, -0.3936, -0.3936,  ..., -0.3936, -0.3936, -0.3936],\n",
              "         [-0.1780, -0.2980, -0.3936,  ..., -0.3936, -0.3936, -0.3936],\n",
              "         [ 0.0772, -0.2939, -0.2480,  ..., -0.3936, -0.3936, -0.3936],\n",
              "         ...,\n",
              "         [-0.3936, -0.3936, -0.3936,  ..., -0.3936, -0.3936, -0.3936],\n",
              "         [-0.3936, -0.3936, -0.3936,  ..., -0.3936, -0.3936, -0.3936],\n",
              "         [-0.3936, -0.3936, -0.3936,  ..., -0.3936, -0.3936, -0.3936]],\n",
              "        device='cuda:0'),\n",
              " 'पालन करने को लेकर आम लोगों में जागरूकता नहीं यहाँ के लोग यातायात नियमों के पालन करने को लेकर गंभीर नही है इसी वजह से आजकल सड़क हादसों में')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "comYI3DN-Ff_"
      },
      "outputs": [],
      "source": [
        "def decode_baseline(model, mel, beam_size):\n",
        "  \"\"\"\n",
        "    This function performs the transcription with Whisper to provide a baseline without LM fusion\n",
        "\n",
        "    - model: The Whisper model used for transcription.\n",
        "    - mel: Represents Mel-spectrogram data, likely input features for the model.\n",
        "    - beam_size: An integer specifying the size of the beam for beam search decoding.\n",
        "  \"\"\"\n",
        "  if mel.shape[0] != 1:\n",
        "    mel = torch.unsqueeze(mel, 0)\n",
        "  options = whisper.DecodingOptions(fp16 = False, beam_size=beam_size, without_timestamps=True, language=\"hi\")\n",
        "  result = whisper.decode(model, mel, options)\n",
        "  result = [r.text for r in result]\n",
        "  return result\n",
        "\n",
        "def decode_deep_fusion(model, mel, beam_size, lm_path, lm_alpha):\n",
        "  \"\"\"\n",
        "    This function performs the deep fusion of the LM with Whisper during the beam search decoding step\n",
        "\n",
        "    - model: The Whisper model used for transcription.\n",
        "    - mel: Represents Mel-spectrogram data, likely input features for the model.\n",
        "    - beam_size: An integer specifying the size of the beam for beam search decoding.\n",
        "    - lm_path: A string representing the path to the language model file used for fusion.\n",
        "    - lm_alpha: A numerical value representing the weight assigned to the language model scores during deep fusion.\n",
        "  \"\"\"\n",
        "  if mel.shape[0] != 1:\n",
        "    mel = torch.unsqueeze(mel, 0)\n",
        "  options = whisper.DecodingOptions(fp16 = False, withlm=True, beam_size=beam_size,\n",
        "        patience=1.0, lm_path=lm_path, lm_alpha=lm_alpha, lm_beta=0.0,\n",
        "        without_timestamps=True, language=\"hi\")\n",
        "  result = whisper.decode(model, mel, options)\n",
        "  result = [r.text for r in result]\n",
        "  return result\n",
        "\n",
        "def decode_shallow_fusion_beam_search(model, mel, beam_size, lm_path, lm_weight, debug=False):\n",
        "  \"\"\"\n",
        "    this function performs shallow fusion using best of N hypothesis (decoding)\n",
        "    by combining the scores of whisper and the language model score (which gets weighted by the lm_weight factor)\n",
        "\n",
        "    - model: The Whisper model used for transcription.\n",
        "    - mel: Represents Mel-spectrogram data, likely input features for the model.\n",
        "    - beam_size: An integer specifying the size of the beam for beam search decoding.\n",
        "    - lm_path: A string representing the path to the language model file used for fusion.\n",
        "    - lm_weight: A numerical value representing the weight assigned to the language model scores during shallow fusion.\n",
        "    - debug: A boolean flag (optional) indicating whether to print debug information (default is False).\n",
        "      Useful for inspecting the outputs with different lm_weights for finding the optimal value for lm_weight\n",
        "\n",
        "  \"\"\"\n",
        "  # if testing with a single utterance without a dataloader\n",
        "  if mel.shape[0] != 1:\n",
        "    mel = torch.unsqueeze(mel, 0)\n",
        "  options = whisper.DecodingOptions(fp16 = False, beam_size=beam_size, return_nbest = True, without_timestamps=True, language=\"hi\")\n",
        "  nbests = whisper.decode(model, mel, options)\n",
        "\n",
        "  lm_model = kenlm.LanguageModel(lm_path)\n",
        "  combined_scores = []\n",
        "\n",
        "  for nbest in nbests:\n",
        "    nbest_with_lm_score = [(c.text, c.avg_logprob, lm_model.score(c.text)) for c in nbest]\n",
        "    combined_score = [(text, (whisper_score) + (lm_score*lm_weight), (whisper_score), (lm_score)) for text, whisper_score, lm_score in nbest_with_lm_score]\n",
        "    combined_score.sort(key=lambda t: t[1], reverse=True)\n",
        "    combined_scores.append(combined_score)\n",
        "\n",
        "\n",
        "  if debug:\n",
        "    print(combined_scores)\n",
        "  # text, final_score, whisper_score, lm_score = combined_scores[0]\n",
        "  # return the highest score element for each input in the batch\n",
        "  result = []\n",
        "  for datapoint in combined_scores:\n",
        "    combined_score = datapoint[0]\n",
        "    hyp, comb_score, w_score, lm_score = combined_score\n",
        "    result.append(hyp)\n",
        "  # result = [hyp for hyp, comb_score, w_score, lm_score in combined_scores]\n",
        "  #return text\n",
        "  return result\n",
        "\n",
        "# def decode_shallow_fusion_nbest(model, mel, best_of, lm_path, temperature, lm_weight, debug=False):\n",
        "#   \"\"\"\n",
        "#     this function performs shallow fusion using best of N hypothesis (greedy decoding)\n",
        "#     by combining the scores of whisper and the language model score (which gets weighted by the lm_weight factor)\n",
        "\n",
        "#     - model: The Whisper model used for transcription.\n",
        "#     - mel: Represents Mel-spectrogram data, likely input features for the model.\n",
        "#     - best_of: An integer specifying the number of best hypotheses to consider during decoding.\n",
        "#     - lm_path: A string representing the path to the language model used for fusion.\n",
        "#     - temperature: A numerical value indicating the temperature parameter used during sampling. Higher temperature corresponds to more variation in the n best list\n",
        "#     - lm_weight: A numerical value representing the weight assigned to the language model scores during shallow fusion.\n",
        "#     - debug: A boolean flag (optional) indicating whether to print debug information (default is False).\n",
        "#       Useful when finding the optimal value for the lm_weight\n",
        "#   \"\"\"\n",
        "#   # if testing with a single utterance without a dataloader\n",
        "#   if mel.shape[0] != 1:\n",
        "#     mel = torch.unsqueeze(mel, 0)\n",
        "#   options = whisper.DecodingOptions(fp16 = False, best_of=best_of, return_nbest=True, without_timestamps=True, temperature=temperature, language=\"hi\")\n",
        "#   nbests = whisper.decode(model, mel, options)\n",
        "\n",
        "#   lm_model = kenlm.LanguageModel(lm_path)\n",
        "#   combined_scores = []\n",
        "\n",
        "#   for nbest in nbests:\n",
        "#     nbest_with_lm_score = [(c.text, c.avg_logprob, lm_model.score(c.text)) for c in nbest]\n",
        "#     combined_score = [(text, (whisper_score) + (lm_score*lm_weight), (whisper_score), (lm_score)) for text, whisper_score, lm_score in nbest_with_lm_score]\n",
        "#     combined_score.sort(key=lambda t: t[1], reverse=True)\n",
        "#     combined_scores.append(combined_score)\n",
        "#   if debug:\n",
        "#     print(combined_scores)\n",
        "#   # text, final_score, whisper_score, lm_score = combined_scores[0]\n",
        "#   # return the highest score element for each input in the batch\n",
        "#   result = []\n",
        "#   for datapoint in combined_scores:\n",
        "#     combined_score = datapoint[0]\n",
        "#     hyp, comb_score, w_score, lm_score = combined_score\n",
        "#     result.append(hyp)\n",
        "#   # result = [hyp for hyp, comb_score, w_score, lm_score in combined_scores]\n",
        "#   #return text\n",
        "#   return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_shallow_fusion_nbest(model, mel, best_of, lm_path, temperature, lm_weight, debug=False):\n",
        "  \"\"\"\n",
        "    this function performs shallow fusion using best of N hypothesis (greedy decoding)\n",
        "    by combining the scores of whisper and the language model score (which gets weighted by the lm_weight factor)\n",
        "\n",
        "    - model: The Whisper model used for transcription.\n",
        "    - mel: Represents Mel-spectrogram data, likely input features for the model.\n",
        "    - best_of: An integer specifying the number of best hypotheses to consider during decoding.\n",
        "    - lm_path: A string representing the path to the language model used for fusion.\n",
        "    - temperature: A numerical value indicating the temperature parameter used during sampling. Higher temperature corresponds to more variation in the n best list\n",
        "    - lm_weight: A numerical value representing the weight assigned to the language model scores during shallow fusion.\n",
        "    - debug: A boolean flag (optional) indicating whether to print debug information (default is False).\n",
        "      Useful when finding the optimal value for the lm_weight\n",
        "  \"\"\"\n",
        "  # if testing with a single utterance without a dataloader\n",
        "  if mel.shape[0] != 1:\n",
        "    mel = torch.unsqueeze(mel, 0)\n",
        "  options = whisper.DecodingOptions(fp16 = False, best_of=best_of, return_nbest=True, without_timestamps=True, temperature=temperature, language=\"hi\")\n",
        "  nbests = whisper.decode(model, mel, options)\n",
        "\n",
        "  lm_model = kenlm.LanguageModel(lm_path)\n",
        "  combined_scores = []\n",
        "\n",
        "  for nbest in nbests:\n",
        "        nbest_with_lm_score = [(c.text, c.avg_logprob, lm_model.score(c.text)) for c in nbest]\n",
        "        normalized_scores = [(text,\n",
        "                              whisper_score / len(text.split()),\n",
        "                              lm_model.score(text) / len(text.split()))\n",
        "                             for text, whisper_score, _ in nbest_with_lm_score]\n",
        "        combined_score = [(text,\n",
        "                           (whisper_score ** 2) + (lm_weight * lm_score ** 2),\n",
        "                           whisper_score,\n",
        "                           lm_score)\n",
        "                          for text, whisper_score, lm_score in normalized_scores]\n",
        "        combined_score.sort(key=lambda t: t[1], reverse=False)\n",
        "        combined_scores.append(combined_score)\n",
        "  if debug:\n",
        "    print(combined_scores)\n",
        "  # text, final_score, whisper_score, lm_score = combined_scores[0]\n",
        "  # return the highest score element for each input in the batch\n",
        "  result = []\n",
        "  for datapoint in combined_scores:\n",
        "    combined_score = datapoint[0]\n",
        "    hyp, comb_score, w_score, lm_score = combined_score\n",
        "    result.append(hyp)\n",
        "  # result = [hyp for hyp, comb_score, w_score, lm_score in combined_scores]\n",
        "  #return text\n",
        "  return result"
      ],
      "metadata": {
        "id": "CklLtkFkD8dE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hypotheses = []\n",
        "# references = []\n",
        "\n",
        "# lm_path = '/content/language_model_3p0.bin'\n",
        "# # # decoding parameters you can try playing around with to reach the optimal WER\n",
        "# beam_size = 20\n",
        "# lm_weight = 0.1\n",
        "# whisper_results_with_shallow_fusion_beam_search = {}\n",
        "\n",
        "# # audio = whisper.load_audio(\"/content/audio_1.wav\")\n",
        "# # audio = whisper.pad_or_trim(audio)\n",
        "# # mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "# # # decode_baseline decode_shallow_fusion_beam_search\n",
        "# # result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "# # result\n",
        "\n",
        "# global audio_files\n",
        "# # Transcribe each audio file\n",
        "# for audio_file in audio_files:\n",
        "#     audio = whisper.load_audio(audio_file)\n",
        "#     audio = whisper.pad_or_trim(audio)\n",
        "#     mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "#     result = decode_shallow_fusion_beam_search(model, mel, beam_size=beam_size, lm_path=lm_path, lm_weight=lm_weight,debug=True)\n",
        "#     whisper_results_with_shallow_fusion_beam_search[audio_file] = result\n",
        "\n",
        "# # Print the results\n",
        "# for audio_file, transcription in whisper_results_with_shallow_fusion_beam_search.items():\n",
        "#     print(f\"{audio_file}: {transcription}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9HcFZh9vURd",
        "outputId": "a2e50884-85f7-4987-963d-c2d9a0be8eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('पूजा में', -1.1214643695137718, -0.43847634575583716, -6.829880237579346), ('उगेउगे सूरज छठ पूजा में', -2.1856764639577557, -0.16355369937035344, -20.221227645874023), ('उगे सूरज छठ पूजा में', -2.1871442061204176, -0.2553261976975661, -19.318180084228516), ('उगेुगे सूरज छठ पूजा में', -2.2670618764815793, -0.24493911189417686, -20.221227645874023), ('उगे�गे सूरज छठ पूजा में', -2.277782917022705, -0.2556601524353027, -20.221227645874023), ('उगे उगे सूरज छठ पूजा में', -2.5255433553649533, -0.007628011126672068, -25.179153442382812), ('उगे होगे सूरज छठ पूजा में', -2.6245084639518494, -0.22054613790204447, -24.039623260498047), ('ओगे उगे सूरज छठ पूजा में', -2.7583050143334176, -0.20635970946281187, -25.519453048706055), ('ओगे ओगे सूरज छठ पूजा में', -2.7975419444422567, -0.21156667893932712, -25.859752655029297), ('ऊगे ऊगे सूरज छठ पूजा में', -2.8386132917096543, -0.14008843514227098, -26.985248565673828)]]\n",
            "[[('हमारा नाम चंदन कुमार सिंघ्यार्म', -2.3871874144583036, -0.29216959982207324, -20.950178146362305), ('हमारा नाम चंदन कुमार सिंह और हम राँची से झारखंड स्टेट', -3.479957655856484, -0.0731335439180073, -34.068241119384766), ('हमारा नाम चंदन कुमार सिंह और हम राज्य से झारखंड स्टेट याम', -3.6335639357566833, -0.06302484273910522, -35.70539093017578), ('हमारा नाम चंदन कुमार सिंह और हम रांची से झारखंड स्टेट याम', -3.7821863174438475, -0.06424694061279297, -37.17939376831055), ('हमारा नाम चंदन कुमार सिंह और हम राँची से झारखंड स्टेट याम', -4.0825059617151975, -0.019522639571643265, -40.62983322143555), ('हमारा नाम चंदन कुमार सिंह और हम राँची से झारखंड स्टेट हैम', -4.117163666089375, -0.05236645539601644, -40.647972106933594), ('हमारा नाम चंदन कुमार सिंह और हम राँजी से झारखंड स्टेट याम', -4.144051449416114, -0.049279873488379304, -40.947715759277344), ('हमारा नाम चन्दन कुमार सिंह और हम राँची से झारखंड स्टेट हैम', -4.168357498919378, -0.06565287855804944, -41.02704620361328), ('हमारा नाम चंदन कुमार सिंह और हम राँजी से झारखंड स्टेट हैम', -4.181334042549134, -0.08474857807159424, -40.96585464477539), ('हमारा नाम चंदन कुमार सिंह और हम राँची से झारखंड स्टेट हाम', -4.2260877688725795, -0.09329472382863363, -41.32793045043945)]]\n",
            "[[('आज सुबह में हम थ्रा लेट से उठे हैं कल रात में लेट से सोयें', -4.878691261693051, -0.09045731393914473, -47.88233947753906), ('आज सुबह में हम थ्रा लेट से उठे हैं कल रात में लेट से सोये के', -4.883792477640613, -0.08440321067283893, -47.993892669677734), ('आज सुबह में हम थ्रा लेट से उठें कल रात में लेट से सोयें', -5.1579641602256086, -0.052941729805686254, -51.05022430419922), ('आज सुबह में हम थ्रा लेट से उठें कर रात में लेट से सोयें', -5.295863515680487, -0.07515390569513494, -52.207096099853516), ('आज सुबह में हम थ्रा लेड से उठे हैं कल रात में लेड से सोयें', -5.406194163205331, -0.09106240356177614, -53.15131759643555), ('आज सुबह में हम थ्रा लेड से उठें कल रात में लेड से सोयें', -5.5346216028386905, -0.05152109319513494, -54.83100509643555), ('आज सुबह में हम थ्रा लेड से उठें कर रात में लेड से सोयें', -5.671910996870562, -0.07312292619185014, -55.98788070678711), ('हान आज सुबह में हम थ्रा लेट से उठें कल रात में लेट से सोयें', -5.8230027659186, -0.07030386760317046, -57.5269889831543), ('हान आज सुबह में हम थ्रा लेड से उठें कल रात में लेड से सोयें', -6.197540431187071, -0.06676307217828159, -61.30777359008789), ('हान आज सुबह में हम थ्रा लेड से उठें कर रात में लेड से सोयें', -6.3353356673799714, -0.0888707473360259, -62.46464920043945)]]\n",
            "[[('भारत की वर्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है', -4.217500662385372, -0.05566365677013732, -41.618370056152344), ('भारत की वर्ड कप में हार से सारे भाग के बहुत ही दुःखित है', -4.3780266250882836, -0.05062336581093924, -43.27403259277344), ('भारत की वर्डकप में हाथ से सारे भाग के बहुत ही दुःखित है', -4.411942392483092, -0.048568445339537504, -43.63373947143555), ('भारत की वर्ड कप में हार से सारे भाग के बहुत ही दुःखित हैं', -4.564751136512087, -0.20808800479821993, -43.56663131713867), ('भारत की वर्डकप में हार से सारे भाग के बहुत ही दुःखित है', -4.57140263744763, -0.042462055172239034, -45.289405822753906), ('भारत की वर्चअप में हार से सारे भाग के बहुत ही दुःखित है', -4.585530017551623, -0.05799171799107602, -45.27538299560547), ('भारत की वर्ड कप में हाज से सारे भाग के बहुत ही दुःखित है', -4.6186219165199685, -0.07521905397114001, -45.43402862548828), ('भारत की वर्डकप में हार से सारे भाग के बहुत ही दुःखित हैं', -4.763929671571966, -0.20572921686005174, -45.58200454711914), ('भारत की वर्डकप में हाज से सारे भाग के बहुत ही दुःखित है', -4.81731051394814, -0.0723703284012644, -47.44940185546875), ('भारत की वर्ट कप में हार से सारे भाग के बहुत ही दुःखित है', -4.9571540730340145, -0.08698233536311559, -48.701717376708984)]]\n",
            "[[('बिहार की', -0.8281438430150351, -0.1734227736790975, -6.547210693359375), ('बिहार की है', -1.661565620248968, -0.9374334161931818, -7.241322040557861), ('बिहार में', -1.7137202739715578, -1.2038267135620118, -5.098935604095459), ('बिहार के', -1.7805942058563233, -1.181722640991211, -5.988715648651123), ('बिहार से', -1.9424013137817382, -1.3323497772216797, -6.100515365600586), ('बिहार', -1.9642270496913365, -1.419480460030692, -5.447465896606445), ('बिस्तर', -2.2464224338531493, -1.6120414733886719, -6.343809604644775), ('बिधार की', -2.412798023223877, -1.189061737060547, -12.2373628616333), ('बिआर की', -2.528901047176785, -1.305164761013455, -12.2373628616333), ('की', -5.1751391887664795, -4.767616271972656, -4.075229167938232)]]\n",
            "/content/audio_1_1.wav: ['पूजा में']\n",
            "/content/audio_2_2.wav: ['हमारा नाम चंदन कुमार सिंघ्यार्म']\n",
            "/content/audio_3_3.wav: ['आज सुबह में हम थ्रा लेट से उठे हैं कल रात में लेट से सोयें']\n",
            "/content/audio_4_4.wav: ['भारत की वर्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है']\n",
            "/content/audio_5_5.wav: ['बिहार की']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "best_of = 30\n",
        "temperature = 0.9\n",
        "lm_weight = .75\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "# # decoding parameters you can try playing around with to reach the optimal WER\n",
        "\n",
        "\n",
        "whisper_results_with_shallow_fusion_greedy_decoding = {}\n",
        "\n",
        "# audio = whisper.load_audio(\"/content/audio_1.wav\")\n",
        "# audio = whisper.pad_or_trim(audio)\n",
        "# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "# # decode_baseline decode_shallow_fusion_beam_search\n",
        "# result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "# result\n",
        "\n",
        "global audio_files\n",
        "# Transcribe each audio file\n",
        "for audio_file in audio_files:\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "    result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=True)\n",
        "    whisper_results_with_shallow_fusion_greedy_decoding[audio_file] = result\n",
        "\n",
        "# Print the results\n",
        "for audio_file, transcription in whisper_results_with_shallow_fusion_greedy_decoding.items():\n",
        "    print(f\"{audio_file}: {transcription}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkor9lKhwOXS",
        "outputId": "baa4b3a9-9728-44da-c802-c063b7e0b515"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('उगे हुए सूरज छठ पूजा में', 8.297486779325695, -0.003981083951970583, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.297486779325695, -0.003981083951970583, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.297486779325695, -0.003981083951970583, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.297486779330798, -0.003981084592880741, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.297486779330798, -0.003981084592880741, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.297486779392035, -0.0039810922838026475, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.297486779392035, -0.0039810922838026475, -3.326153119405111), ('उगे उगे सूरज छठ पूजा में', 13.208156834468774, -0.006055265344599242, -4.196525573730469), ('उगे उगे सूरज छठ पूजा में', 13.208156834468774, -0.006055265344599242, -4.196525573730469), ('उगे उगे सूरज छठ पूजा में', 13.208156834530868, -0.006055270471880513, -4.196525573730469)]]\n",
            "[[('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडियम है', 6.572148691040659, -0.000571700630808723, -2.960213587834285), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडियम है', 6.572148691042272, -0.0005717020415695461, -2.960213587834285), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडियम है', 6.572148691042272, -0.0005717020415695461, -2.960213587834285), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडियम है', 6.5721486910424325, -0.0005717021826456284, -2.960213587834285), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडिया में', 8.025014739947512, -0.002320884268878501, -3.271087353046124), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेट हेम आई', 8.273348252675124, -0.008711558669360716, -3.3212994166782925), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडिया', 8.559652050549033, -0.0032571686638726127, -3.3782917658487954), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडिया', 8.55965205055725, -0.003257169925346576, -3.3782917658487954), ('हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेट हामारा', 8.688975945000449, -0.0076231950924390965, -3.403707357553335), ('हमारा नाम चंदन कुमार सिंह है और हमरांची से झारखण्ड स्टेडिया में', 10.168525072398117, -0.007723670739393968, -3.682112693786621)]]\n",
            "[[('आज सुबह में हम थ्रा लेट से उठें दर रात में लेट से सोए के', 9.363647452550591, -0.0070666012011076276, -3.5333831787109373), ('आज सुबह में हम थ्रा लेट से उठें दल रात में लेट से सोए के', 9.535773797810217, -0.0030487445362827235, -3.5657190958658855), ('आज सुबह में हम थ्रा लेट से उठें दल रात में लेट से सोए के', 9.535773797811917, -0.0030487448151348628, -3.5657190958658855), ('आज सुबह में हम थ्रा लेट से उठे तरदात में लेट से सोए के', 9.57862961037917, -0.005637486978453033, -3.573718479701451), ('आज सुबह में हम थोड़े से उठे तरदात में लेट से सोए के', 10.448371002608193, -0.009238820104203986, -3.732431851900541), ('आज सुबह में हम थ्रा लीड से उठें दल रात में लीड से सोए के', 10.980104375349079, -0.006787293417411938, -3.8262354532877603), ('आज सुबह में हम थ्रा लेट से उठें दर्रात में लेट से सोचे', 11.027028281517678, -0.010665414752541007, -3.834390786977915), ('आज सुबह में हम थ्रा लेट से उठें दर्रात में लेट से सोए के', 11.150354528812692, -0.003943121821659251, -3.855790274483817), ('आज सुबह में हम थ्रा लेट से उठें दर्रात में लेट से सोए के', 11.150354528815047, -0.0039431221204294, -3.855790274483817), ('आज सुबह में हम थ्रा लेट से उठें तरदात में लेट से सोए के', 11.150361640823062, -0.004760275209756722, -3.855790274483817)]]\n",
            "[[('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है', 6.3025611785066005, -0.00238228825622263, -2.8988631112234935), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है', 6.3025611785066005, -0.00238228825622263, -2.8988631112234935), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है', 6.302561178514852, -0.002382289988076716, -2.8988631112234935), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983778433, -0.0005861006882555204, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983778871, -0.000586101061718207, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983778959, -0.0005861011364107443, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983778959, -0.0005861011364107443, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983780798, -0.0005861027049540279, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है।', 8.006168345960926, -0.004342481179873893, -3.2672413417271207), ('भारत की वर्ल्ड कंप में हाथ से सारे भाग के बहुत ही दुखित है', 8.350413950709598, -0.0027329683891070887, -3.3367462158203125)]]\n",
            "[[('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771523543, -0.0020599220250103927, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771523543, -0.0020599220250103927, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771523543, -0.0020599220250103927, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771527335, -0.002059922945545447, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771527335, -0.002059922945545447, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771528284, -0.0020599231756792103, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771528284, -0.0020599231756792103, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है।', 8.788151488810282, -0.012618013375312194, -3.4230575561523438), ('बिहार की स्थिति थोड़ी अच्छी नहीं है।', 13.847752119408524, -0.0038310145283793353, -4.296934945242746), ('बिहार की स्थिति थोड़ी अच्छी नहीं है।', 13.847752119408524, -0.0038310145283793353, -4.296934945242746)]]\n",
            "/content/audio_1_1.wav: ['उगे हुए सूरज छठ पूजा में']\n",
            "/content/audio_2_2.wav: ['हमारा नाम चंदन कुमार सिंह है और हम रांची से झारखण्ड स्टेडियम है']\n",
            "/content/audio_3_3.wav: ['आज सुबह में हम थ्रा लेट से उठें दर रात में लेट से सोए के']\n",
            "/content/audio_4_4.wav: ['भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुःखित है']\n",
            "/content/audio_5_5.wav: ['बिहार की स्थिति थोड़ी अच्छी नहीं है']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding with Default whisper **"
      ],
      "metadata": {
        "id": "J6NqXssrHIOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load the model\n",
        "model_orignal_whisper = whisper.load_model(\"small\")"
      ],
      "metadata": {
        "id": "2yTC3rutHwe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Code to perform call compute metrics on test dataset using our final model and calculate the WER and CER\n",
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "\n",
        "# decoding parameters you can try playing around with to reach the optimal WER\n",
        "best_of = 10\n",
        "temperature = 0.3\n",
        "lm_weight = 0.01\n",
        "\n",
        "for mel, text in tqdm(loader):\n",
        "    # results = model.decode(mels, options)\n",
        "    # hypotheses.extend([result.text for result in results])\n",
        "    result = decode_shallow_fusion_nbest(model_orignal_whisper, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "    hypotheses.extend(result)\n",
        "    references.extend(text)\n",
        "\n",
        "\n",
        "shallow_fusion_nbest_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "#shallow_fusion_nbest_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hYkPWzmH1d3",
        "outputId": "4c744da6-25c4-4ac7-a710-62084d97337b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:11<00:00,  6.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import jiwer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to calculate WER and CER\n",
        "def compute_metrics(model, loader, beam_size, lm_weight, lm_path):\n",
        "    hypotheses = []\n",
        "    references = []\n",
        "\n",
        "    for mel, text in tqdm(loader):\n",
        "        result = decode_shallow_fusion_beam_search(model, mel, beam_size=beam_size, lm_path=lm_path, lm_weight=lm_weight)\n",
        "        hypotheses.extend(result)\n",
        "        references.extend(text)\n",
        "\n",
        "    extra_chars = '‘’“”।'\n",
        "    reference = [text.translate(str.maketrans('', '', string.punctuation + extra_chars)) for text in references]\n",
        "    hypo = [text.translate(str.maketrans('', '', string.punctuation + extra_chars)) for text in hypotheses]\n",
        "\n",
        "    wer = jiwer.wer(reference, hypo)\n",
        "    cer = jiwer.cer(reference, hypo)\n",
        "    return wer, cer\n",
        "\n",
        "# Tuning parameters\n",
        "beam_sizes = [5, 10, 15, 20]\n",
        "lm_weights = [0.01, 0.05, 0.1, 0.15,0.2,0.3,0.4,0.5]\n",
        "\n",
        "best_wer = float('inf')\n",
        "best_params = {'beam_size': None, 'lm_weight': None}\n",
        "\n",
        "for beam_size in beam_sizes:\n",
        "    for lm_weight in lm_weights:\n",
        "        wer, cer = compute_metrics(model_orignal_whisper, loader, beam_size, lm_weight, lm_path)\n",
        "        print(f\"Beam size: {beam_size}, LM Weight: {lm_weight}, WER: {wer * 100:.2f} %, CER: {cer * 100:.2f} %\")\n",
        "\n",
        "        if wer < best_wer:\n",
        "            best_wer = wer\n",
        "            best_params['beam_size'] = beam_size\n",
        "            best_params['lm_weight'] = lm_weight\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best WER: {best_wer * 100:.2f} % with Beam Size: {best_params['beam_size']} and LM Weight: {best_params['lm_weight']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "habDRn3NI2YU",
        "outputId": "54167cf8-3519-43d4-c892-72c7b842cc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:37<00:00,  3.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.01, WER: 87.06 %, CER: 60.38 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:35<00:00,  3.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.05, WER: 99.65 %, CER: 78.47 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:34<00:00,  3.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.1, WER: 90.91 %, CER: 65.59 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:36<00:00,  3.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.15, WER: 89.16 %, CER: 67.42 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:34<00:00,  3.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.2, WER: 89.86 %, CER: 63.41 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:32<00:00,  2.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.3, WER: 90.21 %, CER: 66.36 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:32<00:00,  2.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.4, WER: 90.91 %, CER: 65.66 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:31<00:00,  2.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 5, LM Weight: 0.5, WER: 86.01 %, CER: 62.77 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:55<00:00,  5.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.01, WER: 82.52 %, CER: 58.62 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:52<00:00,  4.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.05, WER: 82.17 %, CER: 59.47 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:53<00:00,  4.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.1, WER: 84.27 %, CER: 58.90 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:57<00:00,  5.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.15, WER: 82.52 %, CER: 58.90 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:58<00:00,  5.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.2, WER: 84.97 %, CER: 60.94 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:54<00:00,  5.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.3, WER: 81.82 %, CER: 57.85 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:59<00:00,  5.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.4, WER: 81.12 %, CER: 55.17 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:54<00:00,  4.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 10, LM Weight: 0.5, WER: 84.97 %, CER: 60.03 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:21<00:00,  7.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.01, WER: 84.97 %, CER: 63.27 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:24<00:00,  7.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.05, WER: 86.71 %, CER: 63.48 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:19<00:00,  7.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.1, WER: 85.66 %, CER: 64.18 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:16<00:00,  6.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.15, WER: 87.41 %, CER: 65.45 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:10<00:00,  6.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.2, WER: 86.36 %, CER: 67.21 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:25<00:00,  7.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.3, WER: 87.76 %, CER: 67.21 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:13<00:00,  6.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.4, WER: 86.01 %, CER: 66.71 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:14<00:00,  6.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 15, LM Weight: 0.5, WER: 86.01 %, CER: 66.85 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:41<00:00,  9.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.01, WER: 88.46 %, CER: 72.48 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:41<00:00,  9.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.05, WER: 87.06 %, CER: 67.35 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:41<00:00,  9.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.1, WER: 89.16 %, CER: 72.41 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:40<00:00,  9.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.15, WER: 86.71 %, CER: 68.12 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:41<00:00,  9.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.2, WER: 86.36 %, CER: 68.33 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:46<00:00,  9.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.3, WER: 87.06 %, CER: 70.16 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:43<00:00,  9.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.4, WER: 87.76 %, CER: 73.33 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [01:46<00:00,  9.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam size: 20, LM Weight: 0.5, WER: 89.16 %, CER: 72.27 %\n",
            "Best WER: 81.12 % with Beam Size: 10 and LM Weight: 0.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zZAPa4grISQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZMfw_kPIVXh",
        "outputId": "3b9e4d99-58e8-4ada-b714-a97d30518434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 159.79 %\n",
            "CER: 100.28 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzkjrcWnZTIE"
      },
      "source": [
        "# Decoding without LM as a baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pP3luWRsuxxh",
        "outputId": "8785d633-2b2f-41c8-f0c3-11f7e9283864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:34<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           hypothesis  \\\n",
              "0                                  यही राम सब ही समझा   \n",
              "1                                      राम शपथ में की   \n",
              "2     यह गोरखपुर जनपद तथा गोरखपुर मंडल का मुख्यालय है   \n",
              "3                          शायद दूसरे लोगों के काम आए   \n",
              "4   विपीढ़िया सदस्य पूर्णिमा वर्मन और अरविंदन और व...   \n",
              "5   पसंद आए तो आप उन्हें वापस पुराने रूप में ले सक...   \n",
              "6                   हर को कम से कम एक श्रेणी में रखें   \n",
              "7                                                  आप   \n",
              "8   प्रधानमंत्री नरेन्द्र मोदी ने कहा है कि प्रधान...   \n",
              "9   अन्य व्यंजन शब्द के अंत में आते हैं तो ज्यादात...   \n",
              "10       आपकी सुविधा के लिए एक सूची मैं भी दे रहा हूँ   \n",
              "11                  मैने पर्यटन में कुछ बदलाव किए हैं   \n",
              "12                           भारत वचन सुनी देखी सुभाऊ   \n",
              "13                   शाही परिवार के सदस्यों में से था   \n",
              "14         यह जिला अभी भी वायु मार्ग से नहीं जुड़ा है   \n",
              "15                           अफ्रीका के कुछ देशों में   \n",
              "16                                   जान सुना सिख भाई   \n",
              "17  वे लोग अपने मूल निवास स्थान की हिंदी की बोली ब...   \n",
              "18                     इनके बारे में वह जानकारी भी है   \n",
              "19                                               आपका   \n",
              "20                                         ध्यान देना   \n",
              "21  भारतवर्ष के उत्तरी राज्य उत्तर प्रदेश का एक नग...   \n",
              "22            आज मैंने होली को निर्वाचित लेख बनाया है   \n",
              "23              लगता है कि असल में दोनों भाग एक ही है   \n",
              "24                     यह है इसलिए निम्न श्रेणी का है   \n",
              "25                              वह शीर्षक शब्द ही रहा   \n",
              "26                      ब्रह्मा विष्णु रुद्र सुर जेते   \n",
              "27                          लेख जिन पर मैं काम करूंगा   \n",
              "28                           भारत देश के उत्तर प्रदेश   \n",
              "29            यद्यपि वहां से उन्होंने बीए पास कर लिया   \n",
              "\n",
              "                                            reference  \n",
              "0                           एहि बिधि राम सबहि समुझावा  \n",
              "1                             राम सपथ मैं कीन्ह न काऊ  \n",
              "2    यह गोरखपुर जनपद तथा गोरखपुर मण्डल का मुख्यालय है  \n",
              "3                    शायद दूसरे नए लोगों के भी काम आए  \n",
              "4   विकिपीडिया सदस्य पूर्णिमा वर्मन और अरविन्दन् औ...  \n",
              "5   न पसंद आएँ तो आप उन्हें वापस पुराने रुप में ले...  \n",
              "6                 हर लेख को कम से कम एक श्रेणी मे रखे  \n",
              "7          आपकी पसंद के विषयों को जानकर प्रसन्नता हुई  \n",
              "8             हम आपके सुन्दर भविष्य की कामना करते हैं  \n",
              "9   अन्य व्यंजन जब शब्द के अंत में आते हैं तो ज़्य...  \n",
              "10       आपकी सुविधा के लिये एक सूची मै भी दे रहा हूं  \n",
              "11           मैंने पर्यटन भूगोल में कुछ बदलाव किए हैं  \n",
              "12                            भरत बचन सुनि देखि सुभाऊ  \n",
              "13                वह शाही परिवार के सदस्यों में से था  \n",
              "14          यह जिला अभी भी वायु मार्ग से नहीं जुडा है  \n",
              "15  अफ्रीका के कुछ देशों में सोलह मार्च को सूर्य क...  \n",
              "16                         अस जियँ जानि सुनहु सिख भाई  \n",
              "17  वे लोग अपने मूल निवास स्थान की हिन्दी की बोली ...  \n",
              "18                   इनके बारे में वहां जानकारी भी है  \n",
              "19           आप का योगदान वास्तव मै बहुत प्रशंसनीय है  \n",
              "20  ध्यान दिलाने के लिए शुक्रिया मैने सुधार कर दिय...  \n",
              "21  भारतवर्ष के उत्तरी राज्य उत्तर प्रदेश का एक नग...  \n",
              "22             आज मैने होली को निर्वाचित लेख बनाया है  \n",
              "23       लगता है कि असल में दोनों भाग एक ही अध्याय है  \n",
              "24                यह हरा है इसलिये निम्न श्रेणी का है  \n",
              "25                            वहाँ शीर्षक शब्द ही रहा  \n",
              "26                      ब्रह्मा विष्णु रुद्र सुर जेते  \n",
              "27                          लेख जिन पर मैं काम करूँगा  \n",
              "28  भारत देश के उत्तर प्रदेश राज्य के आगरा जिले का...  \n",
              "29          यद्यपि वहाँ से उन्होंने बी. ए पास कर लिया  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-704414d2-015a-4cad-b0ab-bfe13f85b497\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>reference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>यही राम सब ही समझा</td>\n",
              "      <td>एहि बिधि राम सबहि समुझावा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>राम शपथ में की</td>\n",
              "      <td>राम सपथ मैं कीन्ह न काऊ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>यह गोरखपुर जनपद तथा गोरखपुर मंडल का मुख्यालय है</td>\n",
              "      <td>यह गोरखपुर जनपद तथा गोरखपुर मण्डल का मुख्यालय है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>शायद दूसरे लोगों के काम आए</td>\n",
              "      <td>शायद दूसरे नए लोगों के भी काम आए</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>विपीढ़िया सदस्य पूर्णिमा वर्मन और अरविंदन और व...</td>\n",
              "      <td>विकिपीडिया सदस्य पूर्णिमा वर्मन और अरविन्दन् औ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>पसंद आए तो आप उन्हें वापस पुराने रूप में ले सक...</td>\n",
              "      <td>न पसंद आएँ तो आप उन्हें वापस पुराने रुप में ले...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>हर को कम से कम एक श्रेणी में रखें</td>\n",
              "      <td>हर लेख को कम से कम एक श्रेणी मे रखे</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>आप</td>\n",
              "      <td>आपकी पसंद के विषयों को जानकर प्रसन्नता हुई</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>प्रधानमंत्री नरेन्द्र मोदी ने कहा है कि प्रधान...</td>\n",
              "      <td>हम आपके सुन्दर भविष्य की कामना करते हैं</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>अन्य व्यंजन शब्द के अंत में आते हैं तो ज्यादात...</td>\n",
              "      <td>अन्य व्यंजन जब शब्द के अंत में आते हैं तो ज़्य...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>आपकी सुविधा के लिए एक सूची मैं भी दे रहा हूँ</td>\n",
              "      <td>आपकी सुविधा के लिये एक सूची मै भी दे रहा हूं</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>मैने पर्यटन में कुछ बदलाव किए हैं</td>\n",
              "      <td>मैंने पर्यटन भूगोल में कुछ बदलाव किए हैं</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>भारत वचन सुनी देखी सुभाऊ</td>\n",
              "      <td>भरत बचन सुनि देखि सुभाऊ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>शाही परिवार के सदस्यों में से था</td>\n",
              "      <td>वह शाही परिवार के सदस्यों में से था</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>यह जिला अभी भी वायु मार्ग से नहीं जुड़ा है</td>\n",
              "      <td>यह जिला अभी भी वायु मार्ग से नहीं जुडा है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>अफ्रीका के कुछ देशों में</td>\n",
              "      <td>अफ्रीका के कुछ देशों में सोलह मार्च को सूर्य क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>जान सुना सिख भाई</td>\n",
              "      <td>अस जियँ जानि सुनहु सिख भाई</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>वे लोग अपने मूल निवास स्थान की हिंदी की बोली ब...</td>\n",
              "      <td>वे लोग अपने मूल निवास स्थान की हिन्दी की बोली ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>इनके बारे में वह जानकारी भी है</td>\n",
              "      <td>इनके बारे में वहां जानकारी भी है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>आपका</td>\n",
              "      <td>आप का योगदान वास्तव मै बहुत प्रशंसनीय है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>ध्यान देना</td>\n",
              "      <td>ध्यान दिलाने के लिए शुक्रिया मैने सुधार कर दिय...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>भारतवर्ष के उत्तरी राज्य उत्तर प्रदेश का एक नग...</td>\n",
              "      <td>भारतवर्ष के उत्तरी राज्य उत्तर प्रदेश का एक नग...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>आज मैंने होली को निर्वाचित लेख बनाया है</td>\n",
              "      <td>आज मैने होली को निर्वाचित लेख बनाया है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>लगता है कि असल में दोनों भाग एक ही है</td>\n",
              "      <td>लगता है कि असल में दोनों भाग एक ही अध्याय है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>यह है इसलिए निम्न श्रेणी का है</td>\n",
              "      <td>यह हरा है इसलिये निम्न श्रेणी का है</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>वह शीर्षक शब्द ही रहा</td>\n",
              "      <td>वहाँ शीर्षक शब्द ही रहा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>ब्रह्मा विष्णु रुद्र सुर जेते</td>\n",
              "      <td>ब्रह्मा विष्णु रुद्र सुर जेते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>लेख जिन पर मैं काम करूंगा</td>\n",
              "      <td>लेख जिन पर मैं काम करूँगा</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>भारत देश के उत्तर प्रदेश</td>\n",
              "      <td>भारत देश के उत्तर प्रदेश राज्य के आगरा जिले का...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>यद्यपि वहां से उन्होंने बीए पास कर लिया</td>\n",
              "      <td>यद्यपि वहाँ से उन्होंने बी. ए पास कर लिया</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-704414d2-015a-4cad-b0ab-bfe13f85b497')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-704414d2-015a-4cad-b0ab-bfe13f85b497 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-704414d2-015a-4cad-b0ab-bfe13f85b497');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-52a24e57-0f26-42fd-8ef5-bb228becd251\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52a24e57-0f26-42fd-8ef5-bb228becd251')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-52a24e57-0f26-42fd-8ef5-bb228becd251 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_da25eeea-54cf-47d4-92c2-15a1e54b113e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('baseline_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_da25eeea-54cf-47d4-92c2-15a1e54b113e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('baseline_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "##Code to perform call compute metrics on test dataset using our final model and calculate the WER and CER\n",
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "# decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 5\n",
        "\n",
        "for mel, text in tqdm(loader):\n",
        "    # results = model.decode(mels, options)\n",
        "    # hypotheses.extend([result.text for result in results])\n",
        "    result = decode_baseline(model, mel, beam_size)\n",
        "    hypotheses.extend(result)\n",
        "    references.extend(text)\n",
        "\n",
        "baseline_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "baseline_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVpt-nocar5T",
        "outputId": "bc5037d5-5df4-475d-effe-e6adb0c12e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER: 37.59 %\n",
            "CER: 10.83 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "wer = jiwer.wer(list(baseline_df[\"reference\"]), list(baseline_df[\"hypothesis\"]))\n",
        "cer = jiwer.cer(list(baseline_df[\"reference\"]), list(baseline_df[\"hypothesis\"]))\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-DrvdKNZZfV"
      },
      "source": [
        "# Decoding with LM deep fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "dEfO8QAbbdKO",
        "outputId": "422cf661-6683-4d1e-c49e-8b87e104ae5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\n",
            "To: /content/language_model_3p0.bin\n",
            "100%|██████████| 280M/280M [00:03<00:00, 81.6MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'language_model_3p0.bin'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download the binary LM from Gdrive\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\"\n",
        "output = \"language_model_3p0.bin\"\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "HisMIkZzYJjJ",
        "outputId": "0025615a-a8e6-43b1-813c-dd995a0a17b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 6/30 [04:47<19:11, 47.99s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a675691f30ac>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# hypotheses.extend([result.text for result in results])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# result = decode_baseline(model, mel, beam_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_deep_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mhypotheses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-c2138e29980c>\u001b[0m in \u001b[0;36mdecode_deep_fusion\u001b[0;34m(model, mel, beam_size, lm_path, lm_alpha)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         without_timestamps=True, language=\"hi\")\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(model, mel, options, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecodingTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msingle\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, mel)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# call the main sampling loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_logprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_speech_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;31m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36m_main_loop\u001b[0;34m(self, audio_features, tokens)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                 \u001b[0;31m# expand the tokens tensor with the selected next tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                 \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_logprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcompleted\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/decoding.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, tokens, logits, sum_logprobs)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_vocab\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1502\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                         \u001b[0mnew_token_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkenlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                         \u001b[0mnew_token_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_token_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                         \u001b[0mlm_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_token_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "##Code to perform call compute metrics on test dataset using our final model and calculate the WER and CER\n",
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "\n",
        "# decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 20\n",
        "lm_alpha = 0.5\n",
        "\n",
        "for mel, text in tqdm(loader):\n",
        "    # results = model.decode(mels, options)\n",
        "    # hypotheses.extend([result.text for result in results])\n",
        "    # result = decode_baseline(model, mel, beam_size)\n",
        "    result = decode_deep_fusion(model, mel, beam_size=beam_size, lm_path=lm_path, lm_alpha=lm_alpha)\n",
        "    hypotheses.extend(result)\n",
        "    references.extend(text)\n",
        "\n",
        "deep_fusion_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "deep_fusion_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0DWNREhg6ol"
      },
      "source": [
        "# decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 15\n",
        "lm_alpha = 0.5\n",
        "\n",
        "WER: 37.24 %\n",
        "CER: 10.83 %\n",
        "\n",
        "\n",
        "# decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 15\n",
        "lm_alpha = 1.5\n",
        "\n",
        "WER: 37.24 %\n",
        "CER: 10.83 %\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpu45C9BblyC",
        "outputId": "e3ebfdf2-d6c4-4fcc-bd58-78b7d87f3ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WER: 36.55 %\n",
            "CER: 10.76 %\n"
          ]
        }
      ],
      "source": [
        "wer = jiwer.wer(list(deep_fusion_df[\"reference\"]), list(deep_fusion_df[\"hypothesis\"]))\n",
        "cer = jiwer.cer(list(deep_fusion_df[\"reference\"]), list(deep_fusion_df[\"hypothesis\"]))\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8jjEY_BZc0X"
      },
      "source": [
        "# Decoding with shallow fusion (Beam Search)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "V3FQ3GHMIq9h",
        "outputId": "899d9567-0d77-4722-c189-a94995716b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-AspJVZRXcrFMuLKx8C4N7uo9PnQytcB\n",
            "To: /content/language_model_3p0.bin\n",
            "100%|██████████| 280M/280M [00:01<00:00, 199MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'language_model_3p0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQvvnm2IYfK_",
        "outputId": "c9bd471b-48dc-4e72-c826-33f124beaca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:46<00:00,  1.54s/it]\n"
          ]
        }
      ],
      "source": [
        "##Code to perform call compute metrics on test dataset using our final model and calculate the WER and CER\n",
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "\n",
        "# # decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 10\n",
        "lm_weight = 0.1\n",
        "\n",
        "for mel, text in tqdm(loader):\n",
        "    # results = model.decode(mels, options)\n",
        "    # hypotheses.extend([result.text for result in results])\n",
        "    result = decode_shallow_fusion_beam_search(model, mel, beam_size=beam_size, lm_path=lm_path, lm_weight=lm_weight)\n",
        "    hypotheses.extend(result)\n",
        "    references.extend(text)\n",
        "    #exit()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lOP4glqbuEO",
        "outputId": "992f66cf-7da6-4108-ced5-c865df970968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 33.82 %\n",
            "CER: 23.29 %\n"
          ]
        }
      ],
      "source": [
        "import jiwer\n",
        "\n",
        "shallow_fusion_beam_search_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "wer = jiwer.wer(list(shallow_fusion_beam_search_df[\"reference\"]), list(shallow_fusion_beam_search_df[\"hypothesis\"]))\n",
        "cer = jiwer.cer(list(shallow_fusion_beam_search_df[\"reference\"]), list(shallow_fusion_beam_search_df[\"hypothesis\"]))\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b20hHgyUZjJk"
      },
      "source": [
        "# Decoding with shallow fusion (Best of N hypothesis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#after tuning\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import jiwer\n",
        "\n",
        "# Function to calculate WER and CER\n",
        "def compute_metrics(model, loader, best_of, temperature, lm_weight, lm_path):\n",
        "    hypotheses = []\n",
        "    references = []\n",
        "\n",
        "    for mel, text in tqdm(loader):\n",
        "        result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "        hypotheses.extend(result)\n",
        "        references.extend(text)\n",
        "\n",
        "    shallow_fusion_nbest_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "\n",
        "    wer = jiwer.wer(shallow_fusion_nbest_df[\"reference\"].tolist(), shallow_fusion_nbest_df[\"hypothesis\"].tolist())\n",
        "    cer = jiwer.cer(shallow_fusion_nbest_df[\"reference\"].tolist(), shallow_fusion_nbest_df[\"hypothesis\"].tolist())\n",
        "    return wer, cer\n",
        "\n",
        "# Tuning parameters\n",
        "best_of_values = [5]\n",
        "temperature_values = [0.1,0.25,0.5,0.75,0.9]\n",
        "lm_weights = [0.1,0.25,0.5,0.75,0.9]\n",
        "\n",
        "best_wer = float('inf')\n",
        "best_params = {'best_of': None, 'temperature': None, 'lm_weight': None}\n",
        "\n",
        "for best_of in best_of_values:\n",
        "    for temperature in temperature_values:\n",
        "        for lm_weight in lm_weights:\n",
        "            wer, cer = compute_metrics(model, loader, best_of, temperature, lm_weight, lm_path)\n",
        "            print(f\"Best of: {best_of}, Temperature: {temperature}, LM Weight: {lm_weight}, WER: {wer * 100:.2f} %, CER: {cer * 100:.2f} %\")\n",
        "\n",
        "            if wer < best_wer:\n",
        "                best_wer = wer\n",
        "                best_params['best_of'] = best_of\n",
        "                best_params['temperature'] = temperature\n",
        "                best_params['lm_weight'] = lm_weight\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best WER: {best_wer * 100:.2f} % with Best Of: {best_params['best_of']}, Temperature: {best_params['temperature']} and LM Weight: {best_params['lm_weight']}\")\n"
      ],
      "metadata": {
        "id": "wQvigSWQcux2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "b4de1fde-1aff-450c-c8c5-134fed312c8a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:30<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 5, Temperature: 0.1, LM Weight: 0.1, WER: 31.25 %, CER: 20.91 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:30<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 5, Temperature: 0.1, LM Weight: 0.25, WER: 30.51 %, CER: 20.59 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:30<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 5, Temperature: 0.1, LM Weight: 0.5, WER: 30.88 %, CER: 20.67 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:30<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 5, Temperature: 0.1, LM Weight: 0.75, WER: 30.51 %, CER: 20.51 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:30<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 5, Temperature: 0.1, LM Weight: 0.9, WER: 31.62 %, CER: 20.75 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 22/30 [00:22<00:08,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-4a205a866070>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemperature_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlm_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlm_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mwer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_of\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best of: {best_of}, Temperature: {temperature}, LM Weight: {lm_weight}, WER: {wer * 100:.2f} %, CER: {cer * 100:.2f} %\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-4a205a866070>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(model, loader, best_of, temperature, lm_weight, lm_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_shallow_fusion_nbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_of\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mhypotheses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-732ad5767809>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msampling_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sampling_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_or_trim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2019\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best WER: 23.45 % with Best Of: 5, Temperature: 0.1 and LM Weight: 0.1"
      ],
      "metadata": {
        "id": "Gvk9SpZTE6v4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vvti3DsAYSXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from tqdm import tqdm\n",
        "# import jiwer\n",
        "\n",
        "# # Function to calculate WER and CER\n",
        "# def compute_metrics(model, loader, best_of, temperature, lm_weight, lm_path):\n",
        "#     hypotheses = []\n",
        "#     references = []\n",
        "\n",
        "#     for mel, text in tqdm(loader):\n",
        "#         result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "#         hypotheses.extend(result)\n",
        "#         references.extend(text)\n",
        "\n",
        "#     shallow_fusion_nbest_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "\n",
        "#     wer = jiwer.wer(shallow_fusion_nbest_df[\"reference\"].tolist(), shallow_fusion_nbest_df[\"hypothesis\"].tolist())\n",
        "#     cer = jiwer.cer(shallow_fusion_nbest_df[\"reference\"].tolist(), shallow_fusion_nbest_df[\"hypothesis\"].tolist())\n",
        "#     return wer, cer\n",
        "\n",
        "# # Tuning parameters\n",
        "# best_of_values = [5, 10, 15]\n",
        "# temperature_values = [0.2, 0.3, 0.4]\n",
        "# lm_weights = [0.01, 0.05, 0.1,0.2,0.3,0.4,0.5]\n",
        "\n",
        "# best_wer = float('inf')\n",
        "# best_params = {'best_of': None, 'temperature': None, 'lm_weight': None}\n",
        "\n",
        "# for best_of in best_of_values:\n",
        "#     for temperature in temperature_values:\n",
        "#         for lm_weight in lm_weights:\n",
        "#             wer, cer = compute_metrics(model, loader, best_of, temperature, lm_weight, lm_path)\n",
        "#             print(f\"Best of: {best_of}, Temperature: {temperature}, LM Weight: {lm_weight}, WER: {wer * 100:.2f} %, CER: {cer * 100:.2f} %\")\n",
        "\n",
        "#             if wer < best_wer:\n",
        "#                 best_wer = wer\n",
        "#                 best_params['best_of'] = best_of\n",
        "#                 best_params['temperature'] = temperature\n",
        "#                 best_params['lm_weight'] = lm_weight\n",
        "\n",
        "# # Print the best parameters\n",
        "# print(f\"Best WER: {best_wer * 100:.2f} % with Best Of: {best_params['best_of']}, Temperature: {best_params['temperature']} and LM Weight: {best_params['lm_weight']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNEYrVOlcvk3",
        "outputId": "bb6acb1a-dde2-4596-a0e6-3ccd61229c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:30<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.01, WER: 35.63 %, CER: 18.61 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:31<00:00,  2.64s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.05, WER: 36.18 %, CER: 19.26 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.1, WER: 36.07 %, CER: 19.21 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.2, WER: 36.91 %, CER: 18.99 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.3, WER: 36.18 %, CER: 19.22 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.4, WER: 36.51 %, CER: 19.34 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.61s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.2, LM Weight: 0.5, WER: 36.80 %, CER: 19.37 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:30<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.01, WER: 36.40 %, CER: 19.64 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:31<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.05, WER: 37.06 %, CER: 19.89 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.61s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.1, WER: 36.99 %, CER: 19.51 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:31<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.2, WER: 37.98 %, CER: 20.83 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.3, WER: 37.17 %, CER: 20.24 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:30<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.4, WER: 37.54 %, CER: 20.59 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:29<00:00,  2.62s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.3, LM Weight: 0.5, WER: 38.01 %, CER: 20.35 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:31<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.01, WER: 39.52 %, CER: 21.70 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:30<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.05, WER: 38.34 %, CER: 20.75 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:30<00:00,  2.63s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.1, WER: 39.08 %, CER: 21.39 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:31<00:00,  2.64s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.2, WER: 38.45 %, CER: 20.85 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:32<00:00,  2.65s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.3, WER: 39.52 %, CER: 21.29 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:32<00:00,  2.65s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.4, WER: 39.59 %, CER: 21.56 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [04:33<00:00,  2.65s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 5, Temperature: 0.4, LM Weight: 0.5, WER: 39.11 %, CER: 21.16 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:53<00:00,  4.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.01, WER: 38.93 %, CER: 20.69 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:52<00:00,  4.01s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.05, WER: 39.48 %, CER: 21.75 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:54<00:00,  4.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.1, WER: 39.52 %, CER: 21.53 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:54<00:00,  4.03s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.2, WER: 39.30 %, CER: 21.49 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:54<00:00,  4.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.3, WER: 39.44 %, CER: 21.63 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:52<00:00,  4.01s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.4, WER: 38.97 %, CER: 21.28 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:54<00:00,  4.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.2, LM Weight: 0.5, WER: 39.88 %, CER: 22.03 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:55<00:00,  4.04s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.01, WER: 39.48 %, CER: 21.50 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:59<00:00,  4.07s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.05, WER: 39.26 %, CER: 21.39 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:54<00:00,  4.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.1, WER: 38.89 %, CER: 21.05 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:55<00:00,  4.03s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.2, WER: 39.22 %, CER: 21.30 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:56<00:00,  4.04s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.3, WER: 39.52 %, CER: 21.75 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:55<00:00,  4.04s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.4, WER: 39.44 %, CER: 22.04 %\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 103/103 [06:54<00:00,  4.03s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best of: 10, Temperature: 0.3, LM Weight: 0.5, WER: 39.15 %, CER: 21.79 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [06:56<00:00,  4.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.01, WER: 38.97 %, CER: 21.01 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [06:57<00:00,  4.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.05, WER: 38.60 %, CER: 20.55 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [07:00<00:00,  4.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.1, WER: 37.98 %, CER: 20.93 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [06:55<00:00,  4.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.2, WER: 39.30 %, CER: 21.58 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [06:57<00:00,  4.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.3, WER: 39.48 %, CER: 21.64 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [06:56<00:00,  4.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.4, WER: 39.04 %, CER: 21.05 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [07:01<00:00,  4.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 10, Temperature: 0.4, LM Weight: 0.5, WER: 39.19 %, CER: 21.92 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:24<00:00,  5.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.01, WER: 39.04 %, CER: 21.62 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:27<00:00,  5.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.05, WER: 38.97 %, CER: 21.62 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:25<00:00,  5.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.1, WER: 39.74 %, CER: 21.88 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:28<00:00,  5.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.2, WER: 39.66 %, CER: 21.92 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:25<00:00,  5.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.3, WER: 39.63 %, CER: 22.18 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:27<00:00,  5.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.4, WER: 39.04 %, CER: 21.14 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:25<00:00,  5.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.2, LM Weight: 0.5, WER: 39.37 %, CER: 21.68 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:30<00:00,  5.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.01, WER: 39.11 %, CER: 21.34 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:31<00:00,  5.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.05, WER: 39.41 %, CER: 22.05 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:29<00:00,  5.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.1, WER: 38.97 %, CER: 21.21 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:29<00:00,  5.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.2, WER: 39.55 %, CER: 21.95 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:36<00:00,  5.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.3, WER: 39.41 %, CER: 21.77 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:29<00:00,  5.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.4, WER: 38.64 %, CER: 21.16 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:30<00:00,  5.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.3, LM Weight: 0.5, WER: 39.19 %, CER: 21.72 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:32<00:00,  5.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.01, WER: 38.97 %, CER: 21.75 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:33<00:00,  5.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.05, WER: 39.08 %, CER: 21.84 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:38<00:00,  5.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.1, WER: 39.85 %, CER: 22.05 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:32<00:00,  5.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.2, WER: 39.52 %, CER: 21.51 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:31<00:00,  5.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.3, WER: 38.67 %, CER: 20.81 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:36<00:00,  5.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.4, WER: 39.66 %, CER: 22.25 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 103/103 [09:32<00:00,  5.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best of: 15, Temperature: 0.4, LM Weight: 0.5, WER: 38.67 %, CER: 21.42 %\n",
            "Best WER: 35.63 % with Best Of: 5, Temperature: 0.2 and LM Weight: 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Code to perform call compute metrics on test dataset using our final model and calculate the WER and CER\n",
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "\n",
        "# decoding parameters you can try playing around with to reach the optimal WER\n",
        "#best_of = 10\n",
        "#temperature = 0.3\n",
        "#lm_weight = 0.01\n",
        "#WER: 24.31 % , CER: 10.69 %\n",
        "\n",
        "#best_of = 30 temperature = 0.5 lm_weight = .5 WER: 20.00 % CER: 6.89 %\n",
        "#Best WER: 23.45 % with Best Of: 5, Temperature: 0.1 and LM Weight: 0.1\n",
        "best_of = 5\n",
        "temperature = 0.9\n",
        "lm_weight = 0.75\n",
        "\n",
        "#Best Of: 5, Temperature: 0.2 and LM Weight: 0.01\n",
        "\n",
        "for mel, text in tqdm(loader):\n",
        "    # results = model.decode(mels, options)\n",
        "    # hypotheses.extend([result.text for result in results])\n",
        "    hypotheses.extend(decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False))\n",
        "    references.extend(text)\n",
        "\n",
        "\n",
        "shallow_fusion_nbest_df = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
        "#shallow_fusion_nbest_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KV7bMFSRDAlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4651f63a-b3b1-4506-b35a-5c7572fd469c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [09:11<00:00,  1.10s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "wer = jiwer.wer(shallow_fusion_nbest_df[\"reference\"].to_list(),shallow_fusion_nbest_df[\"hypothesis\"].to_list())\n",
        "cer = jiwer.cer(shallow_fusion_nbest_df[\"reference\"].to_list(),shallow_fusion_nbest_df[\"hypothesis\"].to_list())\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ],
      "metadata": {
        "id": "STboVmzyilu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f860f585-1d2f-4da2-ae1f-0b0189d3cc8a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 13.62 %\n",
            "CER: 8.86 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "graminVoice : WER: 30.07 %\n",
        "CER: 14.57 %"
      ],
      "metadata": {
        "id": "wwYAKwAUbFV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IIITB\n",
        "WER: 28.24 %\n",
        "CER: 18.93 %"
      ],
      "metadata": {
        "id": "oylUlXOjY1pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "firefox : WER: 30.69 %\n",
        "CER: 11.48 % with\n",
        "best_of = 5\n",
        "temperature = 0.9\n",
        "lm_weight = 0.75"
      ],
      "metadata": {
        "id": "KgOK1LfgXKOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "L81Lrd3iCycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_chars = '‘’“”।'\n",
        "reference = [text.translate(str.maketrans('', '', string.punctuation + extra_chars)) for text in shallow_fusion_nbest_df[\"reference\"].to_list()]\n",
        "hypo = [text.translate(str.maketrans('', '', string.punctuation + extra_chars)) for text in shallow_fusion_nbest_df[\"hypothesis\"].to_list()]"
      ],
      "metadata": {
        "id": "q_K3wbY9J0T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-eJUWX5wKysM",
        "outputId": "2ca062b8-16a3-4648-c6db-912197617007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'भारतवर्ष के उत्तरी राज्य उत्तर प्रदेश का एक नगर है'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hypo[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XOz8Lr1sK5hx",
        "outputId": "19f96166-ff9a-4081-e4af-41b4020a0491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'भारत वर्ष के उत्तरी राज्य उत्तर प्रदेश का एक नगर है'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBnxUSwUbyeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a8ca6a-0fde-4405-e569-f26946e89c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 28.24 %\n",
            "CER: 18.72 %\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "wer = jiwer.wer(reference,hypo)\n",
        "cer = jiwer.cer(reference,hypo)\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "wer = jiwer.wer(reference,hypo)\n",
        "cer = jiwer.cer(reference,hypo)\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2d4mG9EArKD",
        "outputId": "192160c2-1828-4926-8c1d-2baf2c5e08b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 26.67 %\n",
            "CER: 19.05 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "wer = jiwer.wer(reference,hypo)\n",
        "cer = jiwer.cer(reference,hypo)\n",
        "\n",
        "print(f\"WER: {wer * 100:.2f} %\")\n",
        "print(f\"CER: {cer * 100:.2f} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evl6HvIdF8ER",
        "outputId": "4cdd1d67-3b1f-4680-9aba-768f0ff5b17a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 37.41 %\n",
            "CER: 18.30 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkA9HqG-nkvq"
      },
      "source": [
        "# **Pointer 7 ## Final code t o push our model to hugging face **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gratepG_lcQP"
      },
      "outputs": [],
      "source": [
        "##Code to push the final model to hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQYFfgqeJY2_"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"language\": \"hi\",\n",
        "    \"model_name\": \"Whisper integrated LM 2011\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-medium\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "09E9_ZIjJesT",
        "outputId": "775aebba-7f12-4e13-9cdf-41b2ed78d45f"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a10d0a24e70c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Whisper' object has no attribute 'push_to_hub'"
          ]
        }
      ],
      "source": [
        "model.push_to_hub(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKtFtijYJmbi"
      },
      "source": [
        "**##Inferences of model vs BenchMarkModels -wave2wec and GoogleUSM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF8NmdKLKDpZ"
      },
      "source": [
        "**##Orignal  Whisper Model without any integration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlk1fACOKBaL",
        "outputId": "73acdef2-593f-494a-9047-b3479be40622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20231106)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.5.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.27.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (17.0.5)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (11.7.91)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->openai-whisper) (0.41.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of audio files\n",
        "audio_files = [\n",
        "    \"/content/audio_1_1.wav\",\n",
        "    \"/content/audio_2.wav\",\n",
        "    \"/content/audio_3_3.wav\",\n",
        "    \"/content/audio_4_4.wav\",\n",
        "    \"/content/audio_5_5.wav\"\n",
        "]"
      ],
      "metadata": {
        "id": "qDI7xPq9s3D6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4f2gCnpLG23",
        "outputId": "65a1783e-cbcd-4662-e70d-32c8df6f3460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/audio_1_1.wav:  उगे उगे सुद़ज चध पुजा में\n",
            "/content/audio_2.wav:  अब राची से है, हमार आप चंदन कमार सिंग है, और जारक्ण्ट श्टेट है हमारा.\n",
            "/content/audio_3_3.wav:  आँ आँ आँ सुबवाय में हम तुर लीट सुट हैं तर आप में लीट सुट हो एक हैं\n",
            "/content/audio_4_4.wav:  बाहरत की वल्कुप में हाँसे सारे बहाग के बहुती दुखित है.\n",
            "/content/audio_5_5.wav:  बिहार की श्थेती तोडी अच्छी नहीं हैं\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "# Load the model\n",
        "model_orignal_whisper = whisper.load_model(\"small\")  # Choose model size as needed\n",
        "\n",
        "\n",
        "\n",
        "# Dictionary to hold results\n",
        "orignal_whisper_results = {}\n",
        "\n",
        "# Transcribe each audio file\n",
        "for audio_file in audio_files:\n",
        "    result = model_orignal_whisper.transcribe(audio_file, language=\"hi\")\n",
        "    orignal_whisper_results[audio_file] = result[\"text\"]\n",
        "\n",
        "# Print the results\n",
        "for audio_file, transcription in orignal_whisper_results.items():\n",
        "    print(f\"{audio_file}: {transcription}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiu-YU40Jtam"
      },
      "source": [
        "[link text](https://)**##Orignal Integrated Whisper Model without fusion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZwwkj6cUDyd",
        "outputId": "69c22e37-d278-4534-a945-cb39dc190338"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'हमारा नाम चंदन कुमार सिंह और हम राँची से झारखंड स्टेट याम'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#inferencing using audio files\n",
        "##Code to perform call compute metrics on test dataset using our final model and calculate the WER and CER\n",
        "\n",
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "\n",
        "# # decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 20\n",
        "lm_weight = 0.05\n",
        "# Dictionary to hold results\n",
        "orignal_whisper_results_without_fusion = {}\n",
        "\n",
        "# audio = whisper.load_audio(\"/content/audio_1.wav\")\n",
        "# audio = whisper.pad_or_trim(audio)\n",
        "# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "# options = whisper.DecodingOptions(fp16 = False, beam_size=5, without_timestamps=True, language=\"hi\")\n",
        "# result = whisper.decode(model, mel, options)\n",
        "# baseline = result.text\n",
        "# baseline\n",
        "\n",
        "# Transcribe each audio file\n",
        "for audio_file in audio_files:\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    options = whisper.DecodingOptions(fp16=False, beam_size=5, without_timestamps=True, language=\"hi\")\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    baseline = result.text\n",
        "\n",
        "    original_whisper_results_without_fusion[audio_file] = baseline\n",
        "\n",
        "# Print the results\n",
        "for audio_file, transcription in original_whisper_results_without_fusion.items():\n",
        "    print(f\"{audio_file}: {transcription}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Orignal Integrated Whisper Model with shallow fusion and beam search**"
      ],
      "metadata": {
        "id": "k0vV7a8Kpnon"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "a1bjiBonW5KE",
        "outputId": "da2eb320-fc21-47bd-fcdf-02cb364804b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-4.190035629272461, -3.0971128290349785, -3.387331008911133, -2.975449244181315, -3.098282814025879, -3.413821220397949, -3.4439942042032876, -3.385819435119629, -3.4123096466064453, -3.4189205169677734]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-bce7a9e32242>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# decode_baseline decode_shallow_fusion_beam_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_shallow_fusion_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlm_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-8724ba735ef7>\u001b[0m in \u001b[0;36mdecode_shallow_fusion_beam_search\u001b[0;34m(model, mel, beam_size, lm_path, lm_weight, debug)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# combined_score = [(text, whisper_score + lm_score*lm_weight, whisper_score, lm_score) for text, whisper_score, lm_score in nbest_with_lm_score]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# combined_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         combined_score = [(text, whisper_score + lm_weight * lm_score_normalized, whisper_score, lm_score_normalized)\n\u001b[0m\u001b[1;32m     71\u001b[0m                           for (text, whisper_score, lm_score_normalized) in zip(nbest_with_lm_score, lm_scores_normalized)]\n\u001b[1;32m     72\u001b[0m         \u001b[0mcombined_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-8724ba735ef7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# combined_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         combined_score = [(text, whisper_score + lm_weight * lm_score_normalized, whisper_score, lm_score_normalized)\n\u001b[0;32m---> 71\u001b[0;31m                           for (text, whisper_score, lm_score_normalized) in zip(nbest_with_lm_score, lm_scores_normalized)]\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mcombined_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mcombined_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "# # decoding parameters you can try playing around with to reach the optimal WER\n",
        "beam_size = 20\n",
        "lm_weight = 0.1\n",
        "whisper_results_with_shallow_fusion_beam_search = {}\n",
        "\n",
        "# audio = whisper.load_audio(\"/content/audio_1.wav\")\n",
        "# audio = whisper.pad_or_trim(audio)\n",
        "# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "# # decode_baseline decode_shallow_fusion_beam_search\n",
        "# result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "# result\n",
        "\n",
        "global audio_files\n",
        "# Transcribe each audio file\n",
        "for audio_file in audio_files:\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "    result = decode_shallow_fusion_beam_search(model, mel, beam_size=beam_size, lm_path=lm_path, lm_weight=lm_weight,debug=True)\n",
        "    whisper_results_with_shallow_fusion_beam_search[audio_file] = result\n",
        "\n",
        "# Print the results\n",
        "for audio_file, transcription in whisper_results_with_shallow_fusion_beam_search.items():\n",
        "    print(f\"{audio_file}: {transcription}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#Orignal Integrated Whisper Model with shallow fusion and greedy decoding:**"
      ],
      "metadata": {
        "id": "oEKn50DSpuO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "best_of = 5\n",
        "temperature = 0.9\n",
        "lm_weight = .75\n",
        "\n",
        "lm_path = '/content/language_model_3p0.bin'\n",
        "# # decoding parameters you can try playing around with to reach the optimal WER\n",
        "\n",
        "\n",
        "whisper_results_with_shallow_fusion_greedy_decoding = {}\n",
        "\n",
        "# audio = whisper.load_audio(\"/content/audio_1.wav\")\n",
        "# audio = whisper.pad_or_trim(audio)\n",
        "# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "# # decode_baseline decode_shallow_fusion_beam_search\n",
        "# result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=False)\n",
        "# result\n",
        "\n",
        "global audio_files\n",
        "# Transcribe each audio file\n",
        "for audio_file in audio_files:\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "    result = decode_shallow_fusion_nbest(model, mel, best_of=best_of, lm_path=lm_path, temperature=temperature, lm_weight=lm_weight, debug=True)\n",
        "    whisper_results_with_shallow_fusion_greedy_decoding[audio_file] = result\n",
        "\n",
        "# Print the results\n",
        "for audio_file, transcription in whisper_results_with_shallow_fusion_greedy_decoding.items():\n",
        "    print(f\"{audio_file}: {transcription}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Xp3eU7jIp3",
        "outputId": "22b91a97-cca3-44ba-eb42-1cd0beb7502c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('उगे हुए सूरज छठ पूजा में', 8.297486779677806, -0.003981128174771545, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.29748677968291, -0.003981128815681704, -3.326153119405111), ('उगे हुए सूरज छठ पूजा में', 8.29748677973394, -0.003981135224783292, -3.326153119405111), ('उगे उगे सूरज छठ पूजा में', 13.208156833816789, -0.006055211508145896, -4.196525573730469), ('उगे उगे सूरज छठ पूजा में', 13.208156833863358, -0.006055215353606849, -4.196525573730469)]]\n",
            "[[('अब रांची से है हमारा चंदन कुमार सिंह है झारखंड स्टेट है हमारा', 6.219314283628755, -0.002035517698482012, -2.8796551044170675), ('अब रांची से है हमारा चंदन कुमार सिंह है झारखण्ड स्टेट है हमारा', 7.361207824803155, -0.003030532541183325, -3.1328791104830227), ('अब रांची से है हमारा चंदन कुमार सिंह है झारखंड़ स्टेट है हमारा', 7.938000203439908, -0.0009298161892382146, -3.253305875338041), ('अब रांची से है हमारा चंदन कुमार सिंह है झारखंड़ स्टेट है हमारा', 7.938000203439908, -0.0009298161892382146, -3.253305875338041), ('अब रांची से है हमारा चंदन कुमार सिंह है झारखंड़ स्टेट है हमारा', 7.93800020344032, -0.0009298164110917312, -3.253305875338041)]]\n",
            "[[('आज सुबह में हम थ्रा लेट से उठे तर रात में लेट से सोए के', 8.1406782378148, -0.0059237366630917505, -3.294569905598958), ('आज सुबह में हम थ्रा लेट से उठें दल रात में लेट से सोएं', 9.591556456605783, -0.0072413044316428045, -3.576125281197684), ('आज सुबह में हम थ्रा लेट से उठें तरदात में लेट से सोए के', 11.150361640953909, -0.004760288953183588, -3.855790274483817), ('आज सुबह में हम थ्रा लेट से उठें दर्रात में लेट से सोएकें', 12.998772606368933, -0.006312527770388981, -4.163129072922927), ('आज सुबह में हम थ्रा लेट से उठेन तरदात में लेट से सोएकें', 13.24698291937872, -0.011625331656053782, -4.202673398531401)]]\n",
            "[[('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983784563, -0.0005861059167331323, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983786226, -0.0005861073358913412, -2.899864741734096), ('भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है', 6.306911983786226, -0.0005861073358913412, -2.899864741734096), ('भारत की वर्ल्ड कंप में हाथ से सारे भाग के बहुत ही दुखित है', 8.350413950647006, -0.0027329569379684372, -3.3367462158203125), ('भारत की वर्ल्ड कंप में हाथ से सारे भाग के बहुत ही दुखित है।', 10.295991129992391, -0.005407733576638358, -3.705124718802316)]]\n",
            "[[('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771556727, -0.002059930079692119, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है', 6.686903771558623, -0.002059930539959646, -2.9859447479248047), ('बिहार की स्थिति थोड़ी अच्छी नहीं है।', 8.788151487531465, -0.01261796270098005, -3.4230575561523438), ('बिहार की स्थिति थोड़ी अच्छी नहीं है।', 13.8477521193483, -0.003831006668426178, -4.296934945242746), ('बिहार की स्थिति थोड़ी अच्छी नहीं है।', 13.8477521193483, -0.003831006668426178, -4.296934945242746)]]\n",
            "/content/audio_1_1.wav: ['उगे हुए सूरज छठ पूजा में']\n",
            "/content/audio_2.wav: ['अब रांची से है हमारा चंदन कुमार सिंह है झारखंड स्टेट है हमारा']\n",
            "/content/audio_3_3.wav: ['आज सुबह में हम थ्रा लेट से उठे तर रात में लेट से सोए के']\n",
            "/content/audio_4_4.wav: ['भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है']\n",
            "/content/audio_5_5.wav: ['बिहार की स्थिति थोड़ी अच्छी नहीं है']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ofgIloKYdf"
      },
      "source": [
        "**##Orignal Wave2wec**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hXopFc9cSJj",
        "outputId": "a582763e-a286-455a-852e-d0adedceed71"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:733: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*******************wave 2 wec transcription***************** \n",
            "CHAMMUSHAA U CHACHA TRU I'L SHAR\n"
          ]
        }
      ],
      "source": [
        "import soundfile as sf\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# Function to read and preprocess the audio file\n",
        "def speech_file_to_array_fn(path):\n",
        "    speech_array, sampling_rate = sf.read(path)\n",
        "    return speech_array\n",
        "\n",
        "# Function to perform inference\n",
        "def asr_transcript(audio_file):\n",
        "    # Load and preprocess the audio\n",
        "    speech = speech_file_to_array_fn(audio_file)\n",
        "    input_values = tokenizer(speech, return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values).logits\n",
        "\n",
        "    # Get predicted IDs and decode them to text\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.batch_decode(predicted_ids)\n",
        "\n",
        "    return transcription[0]\n",
        "\n",
        "# Path to your audio file\n",
        "audio_file = \"/content/audio.wav\"\n",
        "\n",
        "# Perform inference and print the result\n",
        "print(\"*******************wave 2 wec transcription***************** \")\n",
        "print(asr_transcript(audio_file))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jpnOwYUKc15"
      },
      "source": [
        "**Orignal Google USM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "uDfTKL4PeFSy",
        "outputId": "872d629c-0dd1-4433-bb49-4a032f3eb2a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-cloud-speech\n",
            "  Downloading google_cloud_speech-2.22.0-py2.py3-none-any.whl (275 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/275.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.2/275.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (1.22.3)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (1.61.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (2.17.3)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (1.59.2)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (2023.7.22)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-speech) (0.5.0)\n",
            "Installing collected packages: google-cloud-speech\n",
            "Successfully installed google-cloud-speech-2.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Google USM\n",
        "\n",
        "!pip install google-cloud-speech\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbqDGYN2IfwI"
      },
      "source": [
        "##Google USM Transcrption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW125a12CNY9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drivedownloaduplaod-eed71df0371c.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV9bEQ9f52tW",
        "outputId": "8f509d7e-d231-456c-976a-fd0adf2d5610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.58.1)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.7)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.8.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (4.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import librosa\n",
        "# import soundfile as sf\n",
        "\n",
        "# # Load the audio file\n",
        "# audio, sr = librosa.load('/content/audio_5.wav', sr=48000)  # Load with original sample rate (48kHz)\n",
        "\n",
        "# # Resample to 16kHz\n",
        "# audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
        "\n",
        "# # Save the resampled audio\n",
        "# sf.write('/content/audio_5_5.wav', audio_resampled, 16000)\n"
      ],
      "metadata": {
        "id": "XmHAWIjf54zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = '/content/audio_1.wav'\n",
        "output_file = '/content/audio_1_1.wav'\n",
        "original_sample_rate = 48000  # 48kHz\n",
        "target_sample_rate = 16000    # 16kHz"
      ],
      "metadata": {
        "id": "h3bwNZadEw6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resample_audio(input_file, output_file, original_sample_rate, target_sample_rate)\n"
      ],
      "metadata": {
        "id": "VHtTIEgqEr37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S58lF2odEu9x",
        "outputId": "1176d86b-c14f-488d-b7a8-a22ba3735b6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_io.BufferedRandom name='/content/audio_1_1.flac'>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Load the WAV file\n",
        "wav_audio = AudioSegment.from_file(\"/content/audio_1_1.wav\", format=\"wav\")\n",
        "\n",
        "# Export as FLAC\n",
        "wav_audio.export(\"/content/audio_1_1.flac\", format=\"flac\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsdZEHzrePRj",
        "outputId": "3ffb4829-b0eb-4e5a-825c-89b64d5ffc55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"alternatives\": [\n",
            "        {\n",
            "          \"transcript\": \"उदयवीर सूरज छठ पूजा में\",\n",
            "          \"confidence\": 0.80075234\n",
            "        }\n",
            "      ],\n",
            "      \"resultEndTime\": \"4.800s\",\n",
            "      \"languageCode\": \"hi-in\"\n",
            "    }\n",
            "  ],\n",
            "  \"totalBilledTime\": \"5s\",\n",
            "  \"requestId\": \"2022486015774079337\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!curl -s -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
        "    https://speech.googleapis.com/v1/speech:recognize \\\n",
        "    -d @sync-request.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
        "    https://speech.googleapis.com/v1/speech:recognize \\\n",
        "    -d @sync-request.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJmdj8Tx8YP2",
        "outputId": "b252cd9e-5e1b-4411-fe59-6558cbe64a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"alternatives\": [\n",
            "        {\n",
            "          \"transcript\": \"मेरा नाम चंदन कुमार सिंह है और हम रांची से झारखंड स्टेट काम है\",\n",
            "          \"confidence\": 0.84342927\n",
            "        }\n",
            "      ],\n",
            "      \"resultEndTime\": \"6.750s\",\n",
            "      \"languageCode\": \"hi-in\"\n",
            "    }\n",
            "  ],\n",
            "  \"totalBilledTime\": \"7s\",\n",
            "  \"requestId\": \"3225024303261540763\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
        "    https://speech.googleapis.com/v1/speech:recognize \\\n",
        "    -d @sync-request.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hzj8MXQC8fmQ",
        "outputId": "a573dc0a-9f8a-4a61-cf60-bf47bc10750d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"alternatives\": [\n",
            "        {\n",
            "          \"transcript\": \"आज आज सुबह में हम फेवरेट स्वीट हैं कल रात में लेट सोए थे\",\n",
            "          \"confidence\": 0.80344194\n",
            "        }\n",
            "      ],\n",
            "      \"resultEndTime\": \"7.350s\",\n",
            "      \"languageCode\": \"hi-in\"\n",
            "    }\n",
            "  ],\n",
            "  \"totalBilledTime\": \"8s\",\n",
            "  \"requestId\": \"2199201602030567921\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
        "    https://speech.googleapis.com/v1/speech:recognize \\\n",
        "    -d @sync-request.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAinRafn8mBh",
        "outputId": "0e947bea-863e-473b-88f5-6d7b94ba1902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"alternatives\": [\n",
            "        {\n",
            "          \"transcript\": \"भारत की वर्ल्ड कप में हाथ से सारे भाग के बहुत ही दुखित है\",\n",
            "          \"confidence\": 0.8286048\n",
            "        }\n",
            "      ],\n",
            "      \"resultEndTime\": \"6.830s\",\n",
            "      \"languageCode\": \"hi-in\"\n",
            "    }\n",
            "  ],\n",
            "  \"totalBilledTime\": \"7s\",\n",
            "  \"requestId\": \"3756998355448913141\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) \\\n",
        "    https://speech.googleapis.com/v1/speech:recognize \\\n",
        "    -d @sync-request.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saVNaPNc8s7p",
        "outputId": "c57fe8f9-5f1b-4d71-ca9f-d0595cb85d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"results\": [\n",
            "    {\n",
            "      \"alternatives\": [\n",
            "        {\n",
            "          \"transcript\": \"बिहार की स्थिति थोड़ी अच्छी नहीं है\",\n",
            "          \"confidence\": 0.65068185\n",
            "        }\n",
            "      ],\n",
            "      \"resultEndTime\": \"7.370s\",\n",
            "      \"languageCode\": \"hi-in\"\n",
            "    }\n",
            "  ],\n",
            "  \"totalBilledTime\": \"8s\",\n",
            "  \"requestId\": \"5686012418812165510\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ('हमारा नाम चंदन कुमार सिंह और हम राँजी से झारखंड स्टेट हैम',"
      ],
      "metadata": {
        "id": "4LMRTYILlyJY"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "150486309a0540b0a51a96664f78aba7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "24c1c8f49f5a43eab9b1b9f9f0a12fc4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273a08df24d343a7a227d09b9e23ebb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28b858b805f54292a322d13611833c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dd766dafea94135862b6e9e0d9876cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f065f61e9e1443a85aee518670a4fa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_24c1c8f49f5a43eab9b1b9f9f0a12fc4",
            "style": "IPY_MODEL_e543d211badc4d41bd944df904ce0317",
            "value": true
          }
        },
        "455bae593a3240d9ba064c213a90ece9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4928327c81124c60bd8e3aca15942167": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7390656d2d4240969f6202876d1a57c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a0a08417de42e88bdf8ffc693f42b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8b595673f6a4db4a47a25e88ebc2d4e",
            "placeholder": "​",
            "style": "IPY_MODEL_981bc86f56014362942f1fad9d5fe00c",
            "value": "Token is valid (permission: write)."
          }
        },
        "773b457557fd4352a317068d87fa4eef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5806a46c454e0c9c3f1ca5f70d4279": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_455bae593a3240d9ba064c213a90ece9",
            "placeholder": "​",
            "style": "IPY_MODEL_ba5939381af044029e254989149974db",
            "value": "Connecting..."
          }
        },
        "8536aa41d17347508a262bbc9693b7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75a0a08417de42e88bdf8ffc693f42b6",
              "IPY_MODEL_bb853deecb594dad80ae466afa531d75",
              "IPY_MODEL_920da0787e654f6bb9a0c76cc88f0f5c",
              "IPY_MODEL_bbb0d45741c8401b819941a9eda1c1a7"
            ],
            "layout": "IPY_MODEL_150486309a0540b0a51a96664f78aba7"
          }
        },
        "920da0787e654f6bb9a0c76cc88f0f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7390656d2d4240969f6202876d1a57c7",
            "placeholder": "​",
            "style": "IPY_MODEL_f0885c2a4abd4fe6902ca4f7aba7b2a1",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "981bc86f56014362942f1fad9d5fe00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f33f70e05d4897aac484cb940392d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab1471db6c574235ac5967dd80734786": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9c06e19a154517b4ff46c7a52d29d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aed79d83bbe84ba5aa53f601588c4ee8",
            "placeholder": "​",
            "style": "IPY_MODEL_28b858b805f54292a322d13611833c33",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "aed79d83bbe84ba5aa53f601588c4ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b805718a8b76423386511730e0fdeaff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8b595673f6a4db4a47a25e88ebc2d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba507efc55c145419acdec6f0fb62b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_773b457557fd4352a317068d87fa4eef",
            "placeholder": "​",
            "style": "IPY_MODEL_b805718a8b76423386511730e0fdeaff",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ba5939381af044029e254989149974db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba67e8ff964d4073b870e1200a87fd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d6ef36d77a09460d82ce46f28df66665",
            "style": "IPY_MODEL_c16fa1150d12439688fa21af50d995c7",
            "tooltip": ""
          }
        },
        "bb853deecb594dad80ae466afa531d75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f33f70e05d4897aac484cb940392d4",
            "placeholder": "​",
            "style": "IPY_MODEL_4928327c81124c60bd8e3aca15942167",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "bbb0d45741c8401b819941a9eda1c1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_273a08df24d343a7a227d09b9e23ebb4",
            "placeholder": "​",
            "style": "IPY_MODEL_3dd766dafea94135862b6e9e0d9876cf",
            "value": "Login successful"
          }
        },
        "c16fa1150d12439688fa21af50d995c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d6ef36d77a09460d82ce46f28df66665": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e543d211badc4d41bd944df904ce0317": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0885c2a4abd4fe6902ca4f7aba7b2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f96cd823f757434997f49f4b2c7d2a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ab1471db6c574235ac5967dd80734786",
            "placeholder": "​",
            "style": "IPY_MODEL_ff114fffeab840c6a8fab1662d1dddd7",
            "value": ""
          }
        },
        "ff114fffeab840c6a8fab1662d1dddd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5b9cb9c276c4f75a02171493454991d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d56c3eaa62a744609bafdca5acc038e2",
              "IPY_MODEL_25487c440dc849659f612de9b9402617",
              "IPY_MODEL_ca4e3baf7eea408286fea1780ef21ea3"
            ],
            "layout": "IPY_MODEL_c9813f84bcf34d3b885b0d63c6b7ab4c"
          }
        },
        "d56c3eaa62a744609bafdca5acc038e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17dd643568a64cbcbc5e29f32024ec65",
            "placeholder": "​",
            "style": "IPY_MODEL_798de64a6ff849e098e4942920ee612e",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "25487c440dc849659f612de9b9402617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d31eabfda37e47c1a74cee038703a941",
            "max": 967127483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_94e6bc86395e4935a7265cd06b83e5d8",
            "value": 967127483
          }
        },
        "ca4e3baf7eea408286fea1780ef21ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40bc37638c7e41d9989dab6c84b7fd73",
            "placeholder": "​",
            "style": "IPY_MODEL_f53ef68e08ea4507a477a69f09b85b16",
            "value": " 967M/967M [00:45&lt;00:00, 19.7MB/s]"
          }
        },
        "c9813f84bcf34d3b885b0d63c6b7ab4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17dd643568a64cbcbc5e29f32024ec65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798de64a6ff849e098e4942920ee612e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d31eabfda37e47c1a74cee038703a941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e6bc86395e4935a7265cd06b83e5d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40bc37638c7e41d9989dab6c84b7fd73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f53ef68e08ea4507a477a69f09b85b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "856e2a72a2084ddc9d4085fd05a6e1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cfc3a3d32c64f348a0fca84302ea47d",
              "IPY_MODEL_11d586734d9845aeade23b9caf832075",
              "IPY_MODEL_882512b358964dcabc7e85d63b5c1b72"
            ],
            "layout": "IPY_MODEL_4362a4fb3abf49079f3335e2881c063d"
          }
        },
        "3cfc3a3d32c64f348a0fca84302ea47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d488d59187a4022955587690b1fbdf7",
            "placeholder": "​",
            "style": "IPY_MODEL_97b085fd630e4dacb189804b72b08803",
            "value": "config.json: 100%"
          }
        },
        "11d586734d9845aeade23b9caf832075": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a49a0c1293984c5d8280e4666779ffa3",
            "max": 1967,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0375dd78dc34657ba930915e4d5989a",
            "value": 1967
          }
        },
        "882512b358964dcabc7e85d63b5c1b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a13d1784af54f3c824f8b8727ca40ad",
            "placeholder": "​",
            "style": "IPY_MODEL_622675dbed5f4527af81c876df8c2ae7",
            "value": " 1.97k/1.97k [00:00&lt;00:00, 168kB/s]"
          }
        },
        "4362a4fb3abf49079f3335e2881c063d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d488d59187a4022955587690b1fbdf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b085fd630e4dacb189804b72b08803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a49a0c1293984c5d8280e4666779ffa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0375dd78dc34657ba930915e4d5989a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a13d1784af54f3c824f8b8727ca40ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622675dbed5f4527af81c876df8c2ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f3972ee7b0f434ba33cbf95eec21fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa4b88dedb4a4102a79486911876d4b8",
              "IPY_MODEL_c24561369ad3479c8d1eb9e0a35e8c41",
              "IPY_MODEL_79d310d010cd40aeb6352a8e5497844a"
            ],
            "layout": "IPY_MODEL_639f139161e34d70854ca54d5f1881ab"
          }
        },
        "aa4b88dedb4a4102a79486911876d4b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7199049a45fa4747b6304af5b439b99f",
            "placeholder": "​",
            "style": "IPY_MODEL_252f2962a23d4114b4a48caaf7a07d64",
            "value": "model.safetensors: 100%"
          }
        },
        "c24561369ad3479c8d1eb9e0a35e8c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8fe2a8f1413483fbaf9749ac5620aae",
            "max": 966995080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3c5eac8fc954826b134c065fa3413d0",
            "value": 966995080
          }
        },
        "79d310d010cd40aeb6352a8e5497844a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6957ee665f5b499893096a1be87a3c49",
            "placeholder": "​",
            "style": "IPY_MODEL_41fea02c370f45479c55102068e40a4a",
            "value": " 967M/967M [00:02&lt;00:00, 318MB/s]"
          }
        },
        "639f139161e34d70854ca54d5f1881ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7199049a45fa4747b6304af5b439b99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252f2962a23d4114b4a48caaf7a07d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8fe2a8f1413483fbaf9749ac5620aae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3c5eac8fc954826b134c065fa3413d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6957ee665f5b499893096a1be87a3c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41fea02c370f45479c55102068e40a4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de6fd611cf4e4f42a58ef38e297965eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c76d5552c90a45879b94413f385b7d4d",
              "IPY_MODEL_9b2203caca2f4588899ea8902b3095b8",
              "IPY_MODEL_74ea3499b452499bbb105e1ffc3759a9"
            ],
            "layout": "IPY_MODEL_40481571c848462c825a108d588971ca"
          }
        },
        "c76d5552c90a45879b94413f385b7d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a767f004a98942cbb0942122a7b24c8a",
            "placeholder": "​",
            "style": "IPY_MODEL_f4a5b237bf9f4c738df63e75873a1ab9",
            "value": "generation_config.json: 100%"
          }
        },
        "9b2203caca2f4588899ea8902b3095b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c64521ad104a4aaea6725ef2b9f20f",
            "max": 3837,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6102748e8e884ebe9b6b4d17dc2fe83d",
            "value": 3837
          }
        },
        "74ea3499b452499bbb105e1ffc3759a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc9a0bd9671b46b0a214b41273401ebf",
            "placeholder": "​",
            "style": "IPY_MODEL_109ab10b8f704ff5bdc5d10c28a6ad00",
            "value": " 3.84k/3.84k [00:00&lt;00:00, 329kB/s]"
          }
        },
        "40481571c848462c825a108d588971ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a767f004a98942cbb0942122a7b24c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4a5b237bf9f4c738df63e75873a1ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37c64521ad104a4aaea6725ef2b9f20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6102748e8e884ebe9b6b4d17dc2fe83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc9a0bd9671b46b0a214b41273401ebf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "109ab10b8f704ff5bdc5d10c28a6ad00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56ae7c0feb094c40a68dc1ade8fdae12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e39f5af40e7453988c77392971932b3",
              "IPY_MODEL_cbdb0f2adacc466b8a1f38a968aca1b9",
              "IPY_MODEL_05d139435d6b48729f0988faf15fcfe7"
            ],
            "layout": "IPY_MODEL_6b780799ce1a45b6aaea60f24a29db37"
          }
        },
        "5e39f5af40e7453988c77392971932b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da270b27fe44389958b37d944aca19a",
            "placeholder": "​",
            "style": "IPY_MODEL_76b0e310458846e2b16bbf193f101fda",
            "value": "Downloading data: 100%"
          }
        },
        "cbdb0f2adacc466b8a1f38a968aca1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3acbd7a0c4346cba4109fcfdaf2d97b",
            "max": 12463616,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_916092918e17497f834beadea72b51ec",
            "value": 12463616
          }
        },
        "05d139435d6b48729f0988faf15fcfe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df70625253db4688b596586d5dbd276a",
            "placeholder": "​",
            "style": "IPY_MODEL_edd2cc6165c34fc6a14a4add6b823f8b",
            "value": " 12.5M/12.5M [00:00&lt;00:00, 32.2MB/s]"
          }
        },
        "6b780799ce1a45b6aaea60f24a29db37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0da270b27fe44389958b37d944aca19a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76b0e310458846e2b16bbf193f101fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3acbd7a0c4346cba4109fcfdaf2d97b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "916092918e17497f834beadea72b51ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df70625253db4688b596586d5dbd276a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edd2cc6165c34fc6a14a4add6b823f8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eda00a71cfe54282949579d9e3d49f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b5d25c07e054693a3984ad53598a20d",
              "IPY_MODEL_4568fc83f44f4d3ea0556b47af11fa15",
              "IPY_MODEL_cb922683c49f4c6ba7917a129c0e889a"
            ],
            "layout": "IPY_MODEL_bd947ffa601e46e5a3c37b5510f0c01d"
          }
        },
        "1b5d25c07e054693a3984ad53598a20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75824b6cf5394fb3a5c93f7eff93800d",
            "placeholder": "​",
            "style": "IPY_MODEL_d7230460e6924852aa0b8a4822341fca",
            "value": "Generating txt.done.data.utf8 split: "
          }
        },
        "4568fc83f44f4d3ea0556b47af11fa15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e03deeeb820344b3b34d9cc520cc2711",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d1dee23754941eba223a76b4688b9cd",
            "value": 1
          }
        },
        "cb922683c49f4c6ba7917a129c0e889a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4289516c78a4ea4b7080dec1b8b1e33",
            "placeholder": "​",
            "style": "IPY_MODEL_5086b292db6e4e1393df74aa1c4a7a38",
            "value": " 1000/0 [00:00&lt;00:00, 1901.76 examples/s]"
          }
        },
        "bd947ffa601e46e5a3c37b5510f0c01d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75824b6cf5394fb3a5c93f7eff93800d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7230460e6924852aa0b8a4822341fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e03deeeb820344b3b34d9cc520cc2711": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8d1dee23754941eba223a76b4688b9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4289516c78a4ea4b7080dec1b8b1e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5086b292db6e4e1393df74aa1c4a7a38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}